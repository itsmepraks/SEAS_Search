# SEAS Search - GWU Course Search System

A fine-tuned LLM-based question-answering system for GWU Computer Science courses, providing information about course schedules, instructors, descriptions, and availability for Spring 2026.

## Overview

This project fine-tunes Llama 3.1 8B to answer questions about GWU Computer Science courses by combining:
- **Real-time schedule data** from GWU's course schedule system
- **Course descriptions** from the GWU Bulletin
- **Fine-tuned language model** for natural language understanding

## Project Structure

```text
SEAS_Search/
â”œâ”€â”€ nb/                             # Jupyter notebooks for training
â”‚   â”œâ”€â”€ Llama3.1_(8B)-KG-QA-System.ipynb         # Knowledge Graph QA system
â”‚   â”œâ”€â”€ Llama3.1_(8B)-finetuning-optimized.ipynb  # Optimized fine-tuning notebook
â”‚   â”œâ”€â”€ Llama3.1_(8B)-finetuning.ipynb            # Original fine-tuning notebook
â”‚   â””â”€â”€ Meta_Synthetic_Data_Llama3_2_(3B).ipynb   # Synthetic data generation (optional)
â”œâ”€â”€ data/                           # Training data
â”‚   â”œâ”€â”€ spring_2026_courses.csv      # Course schedule data
â”‚   â”œâ”€â”€ bulletin_courses.csv          # Course descriptions
â”‚   â”œâ”€â”€ course_finetune.jsonl         # Formatted training dataset (simple Q&A)
â”‚   â””â”€â”€ course_finetune_kg_rag.jsonl  # KG-RAG training dataset (multi-hop)
â”œâ”€â”€ utils/                          # Data preparation scripts
â”‚   â”œâ”€â”€ scrape_courses.py           # Scrapes schedule + bulletin data
â”‚   â”œâ”€â”€ prepare_dataset.py          # Converts CSV to training format
â”‚   â””â”€â”€ convert_kg_to_json.py       # Converts KG pickle to JSON for frontend
â”œâ”€â”€ app/                            # Next.js app directory
â”œâ”€â”€ components/                     # React components
â”œâ”€â”€ public/                         # Static assets
â”‚   â””â”€â”€ data/                       # Frontend data files (JSON exports)
â””â”€â”€ lib/                            # Utility libraries
```

## Notebooks

### ğŸ§  `Llama3.1_(8B)-KG-QA-System.ipynb`

**Purpose:** Knowledge Graph Question Answering system with multi-hop reasoning

**Features:**
- Knowledge graph construction from course data (courses, professors, topics, prerequisites)
- Automated prerequisite and topic extraction from descriptions
- Graph Attention Network (GAT) framework for graph retrieval
- Multi-hop reasoning question generation
- Retrieval-Augmented Generation (RAG) training format
- Comprehensive evaluation metrics (EM, F1, BLEU, ROUGE)
- End-to-end inference pipeline with graph retrieval

**Usage:**
1. Ensure CSV data files exist (`spring_2026_courses.csv`, `bulletin_courses.csv`)
2. Run all cells sequentially - the notebook builds the graph and generates training data
3. Model saves to `lora_model_kg_qa/` and knowledge graph to `kg_graph.pkl`

**Best for:** Complex queries requiring multi-hop reasoning like "Which courses should I take to prepare for X if I've completed Y?"

### ğŸ¯ `Llama3.1_(8B)-finetuning-optimized.ipynb`

**Purpose:** Optimized fine-tuning with best practices for simple Q&A

**Features:**
- Train/Validation split (80/20) to prevent overfitting
- Early stopping based on validation loss
- Optimized hyperparameters (cosine annealing, lower learning rate)
- Evaluation metrics and checkpointing
- Improved inference with proper stopping criteria

**Usage:**
1. Ensure `course_finetune.jsonl` exists (generated by `prepare_dataset.py`)
2. Run all cells sequentially
3. Model saves to `lora_model_optimized/` and `merged_model_optimized/`

**Best for:** Simple single-hop questions like "Who teaches X?" or "Tell me about CSCI 1012."

### ğŸ“ `Llama3.1_(8B)-finetuning.ipynb` (Original)

**Purpose:** Basic fine-tuning implementation

**Features:**
- Simple training loop
- 3 epochs fixed training
- Basic inference setup

**Note:** Use the optimized version for better results.

### ğŸ”§ `Meta_Synthetic_Data_Llama3_2_(3B).ipynb` (Optional)

**Purpose:** Generate synthetic Q&A pairs from course bulletin

**Features:**
- Uses Llama 3.2 3B to generate additional training data
- Scrapes bulletin descriptions
- Creates question-answer pairs

**Note:** Currently not required - the main pipeline uses real schedule data.

## Backend/Model

The backend consists of a data collection pipeline, two distinct training approaches, and model inference capabilities. Here's how everything fits together:

### Why Two Approaches?

We started with the **simple fine-tuning approach** (`finetuning-optimized.ipynb`) to get a working baseline. This worked well for straightforward questions like "Who teaches CSCI 1012?" or "When is Machine Learning offered?" However, we quickly discovered its limitations:

**What the Simple Fine-tuning Approach Lacked:**
- **No relationship understanding**: The model couldn't understand prerequisite chains or course relationships
- **Single-hop reasoning only**: It could answer questions about individual courses but couldn't traverse relationships between courses
- **No structured knowledge**: Course information was treated as isolated facts rather than a connected graph
- **Limited to simple queries**: Complex questions like "Which courses should I take to prepare for computer vision research if I've completed CSCI 6212?" were beyond its capabilities

**Why We Built the Knowledge Graph QA System:**
Our project goal was to build an intelligent question-answering system with **multi-hop reasoning** capabilities. The simple fine-tuning approach couldn't answer questions that required:
- Following prerequisite chains (e.g., "What's the path from CSCI 1112 to CSCI 6364?")
- Finding intersections (e.g., "Which professors teach prerequisites for both X and Y?")
- Topic-based reasoning (e.g., "What courses cover machine learning and are prerequisites for computer vision courses?")

The **KG-QA System** addresses these limitations by:
1. **Building a knowledge graph** that captures relationships between courses, professors, topics, and prerequisites
2. **Using graph retrieval** to find relevant subgraphs for complex queries
3. **Training with RAG format** that combines graph context with LLM generation
4. **Generating multi-hop questions** automatically from the graph structure

**When to Use Which:**
- **Simple Fine-tuning**: Use for production systems that only need to answer basic questions about individual courses. It's faster, simpler, and requires less computational resources.
- **KG-QA System**: Use when you need to answer complex, multi-hop questions that require understanding relationships and traversing the course graph. This aligns with our research goals of exploring Graph Attention Networks and hybrid retrieval-generation architectures.

### Data Flow

```text
1. Data Collection
   â””â”€â”€ scrape_courses.py
       â”œâ”€â”€ Scrapes GWU course schedule â†’ spring_2026_courses.csv
       â””â”€â”€ Scrapes GWU bulletin â†’ bulletin_courses.csv

2. Data Preparation (Two Paths)
   
   Path A: Simple Fine-tuning
   â””â”€â”€ prepare_dataset.py
       â””â”€â”€ Combines CSVs â†’ course_finetune.jsonl
           â””â”€â”€ Used by: finetuning-optimized.ipynb
   
   Path B: Knowledge Graph QA
   â””â”€â”€ KG-QA-System.ipynb (reads CSVs directly)
       â”œâ”€â”€ Extracts prerequisites & topics
       â”œâ”€â”€ Builds knowledge graph
       â”œâ”€â”€ Generates multi-hop questions
       â””â”€â”€ Creates course_finetune_kg_rag.jsonl

3. Model Training
   â”œâ”€â”€ Simple Fine-tuning: Trains on single-hop Q&A pairs
   â””â”€â”€ KG-QA: Trains with graph context for multi-hop reasoning

4. Inference
   â”œâ”€â”€ Simple: Direct question â†’ answer
   â””â”€â”€ KG-QA: Query â†’ entity extraction â†’ graph retrieval â†’ LLM generation
```

### Components

**Data Collection (`utils/scrape_courses.py`)**
- Scrapes real-time course schedule data from GWU's course system
- Fetches course descriptions from the GWU Bulletin
- Outputs structured CSV files with course information, instructors, schedules, and descriptions

**Data Preparation (`utils/prepare_dataset.py`)**
- Combines schedule and bulletin data
- Generates simple Q&A pairs (5 variations per course)
- Creates `course_finetune.jsonl` for basic fine-tuning

**Simple Fine-tuning Approach (`finetuning-optimized.ipynb`)**
- Loads pre-generated JSONL dataset
- Fine-tunes Llama 3.1 8B with LoRA (Low-Rank Adaptation)
- Optimized with train/validation split, early stopping, and better hyperparameters
- Handles straightforward questions about individual courses

**Knowledge Graph QA Approach (`KG-QA-System.ipynb`)**
- Reads raw CSV files directly
- Extracts prerequisite relationships and topics from course descriptions using NLP
- Constructs a knowledge graph with nodes (courses, professors, topics) and edges (prerequisite, taught_by, covers_topic)
- Generates complex multi-hop reasoning questions automatically
- Implements Graph Attention Network (GAT) framework for graph-based retrieval
- Trains model with Retrieval-Augmented Generation (RAG) format, combining graph context with LLM generation
- Can answer complex queries like "Which courses should I take to prepare for computer vision research if I've completed CSCI 6212?"

### Key Differences

| Feature | Simple Fine-tuning | Knowledge Graph QA |
|---------|-------------------|-------------------|
| **Training Data** | Pre-generated JSONL | Graph-generated during training |
| **Question Types** | Single-hop (simple Q&A) | Multi-hop (complex reasoning) |
| **Graph Structure** | None | Full knowledge graph |
| **Retrieval** | Direct model inference | Graph retrieval + LLM |
| **Use Case** | "Who teaches X?" | "What's the path from X to Y?" |

### Output Files

After training, you'll find:
- **Simple Fine-tuning**: `lora_model_optimized/`, `merged_model_optimized/`
- **KG-QA**: `lora_model_kg_qa/`, `kg_graph.pkl`, `graph_retriever.pkl`

## Quick Start

### 1. Prepare Data

```bash
# Scrape course data
python utils/scrape_courses.py

# Generate training dataset
python utils/prepare_dataset.py
```

This creates:
- `data/spring_2026_courses.csv` - Schedule data
- `data/bulletin_courses.csv` - Course descriptions  
- `data/course_finetune.jsonl` - Training dataset

### 2. Fine-tune Model

1. Open `nb/Llama3.1_(8B)-finetuning-optimized.ipynb`
2. Run all cells
3. Model will be saved after training completes

### 3. Test the Model

The notebook includes inference cells that test queries like:
- "Who Teaches Machine Learning?"
- "What courses are available on Tuesdays?"
- "Tell me about CSCI 1012."

## Data Sources

- **Course Schedule:** [GWU Course Schedule](https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202601&subjId=CSCI)
- **Course Descriptions:** [GWU Bulletin - CSCI](https://bulletin.gwu.edu/courses/csci/)

## Live Demo

ğŸŒ **Deployed Application**: [https://seas-search.vercel.app/](https://seas-search.vercel.app/)

The live demo includes:
- Interactive knowledge graph visualization
- Course data exploration
- Training methodology and results
- Architecture overview
- Key learnings and insights

## Current Status

- âœ… Data scraping pipeline (schedule + bulletin)
- âœ… Dataset preparation and formatting
- âœ… Fine-tuning implementation (original + optimized)
- âœ… Knowledge Graph QA system with multi-hop reasoning
- âœ… Model inference and testing
- âœ… Frontend integration and deployment

## Team

Anurag Dhungana and Prakriti Bista

Last updated: December 2025
