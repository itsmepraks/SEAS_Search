{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/itsmepraks/SEAS_Search/blob/main/notebooks/Llama3.1_(8B)-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_yUY2SbuF_p"
   },
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9FoUFHUJuF_q"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874,
     "referenced_widgets": [
      "40229460c5784b7fa0508d6006a7105b",
      "d732044c22664f1e9b25dd3a214683ea",
      "6808b365f87a464186e9a22926f4d72a",
      "22c68308cdbb4f15881727830d17aab0",
      "210ea863c91744eaae8db72a541771f4",
      "37c2a92f40074f999550af983b6883a2",
      "990b431dfcc04ae4af988d3f9424b2f6",
      "e6968ef0ad544f6db00632329e0a4425",
      "ce7528e204d046f9912559662a95df66",
      "ca16f2d74deb4d1583c41cebb018285e",
      "9e00224e20dc4a7da7b19f02d109daec",
      "6e7fcd19809342c9ac3aaa763d2cebe7",
      "bf4262daa7434441ae43c7e058f7c3fc",
      "96efa1398d054a5a8114fd5641d5e285",
      "d2a1e0703aca4b87ac84184fbdeb0661",
      "edf3ffc2765e455a9b0c81906f022e68",
      "2a7c58c3f86d4eeba58dddde39edf2ef",
      "1fb18f5c3a2f454a83a4478ed6840ae6",
      "02b0c159da7c4268b4a9adaa8ed31d0c",
      "39869aea45b74705873b0907d5567a7f",
      "7437ae81908a48d79dfc26decf4ca359",
      "19949c2219db4d699b40a8e30af20962",
      "942e8de8078f4e13bb77220510d4daeb",
      "ffc079b829b94cc5a27834dbda12fb82",
      "b5ccf4da4c4a4003a70b829fa4c8ca98",
      "3fd1d2101be24b30a843f658d2566a74",
      "f1aa40d512754017a0c8f2ccfd9791fd",
      "71a1936013f24336b29e2703749c3eee",
      "94c22e1a57184e48a2e2d4f1e14bc93c",
      "aaf8efb642a14976ad8f03036e5f7aa2",
      "50e26789ca554a919d82106895e36f7e",
      "bbe3916c705a41c2ba0281b53aff47f6",
      "8d07ae5cc9d44eacb647777b77e286a4",
      "c6af7ba474b24fb19f4c7551949e91c9",
      "567a3b74392c4d2fa70bc3977d3e7870",
      "5bcd1fcaacca45e2a9f673a89647f19b",
      "e087f4f11dd745d7b26ef65c050b4751",
      "52e5a8bc9f484248ae26531197d10245",
      "84b4dc385f624841a06e6e48ad21a61e",
      "5273caf20e2649bb92a262218361d32c",
      "bbc01d904edb4d889067ee3a59c871c1",
      "a76854fa7ba84bee9d1b6bbd2a7d6b6f",
      "2cd624bba19a4f69895172ff241f5f2f",
      "16f9f7dfd2074566b5e710b5a7841cdb",
      "e05b3e5bda364018ac564616a6fe563d",
      "fe3ccef9ec614564b3e52001c14a9c38",
      "553728f4dab34292b1ad849f71c09d37",
      "4ebf2cb4347a415ead361439b463aeb2",
      "436063a627b447f08e9182cdcf479cb7",
      "9cc095f6004f470a867e0e2eea1bb230",
      "f33c97cf2fc348b3af4ed7ba2459d395",
      "72902324f0b2440cb2997d3bad8a06f9",
      "d115562ef29f4cd8a632d2a3d1048279",
      "d3d173fd3ae1429381169802590d9880",
      "b6878b7dd8e34112bea756e4794073d5"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "8360fc8c-6b09-4c27-8e43-3ae973bb574c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.9.0+cu126)\n",
      "    Python  3.12.9 (you have 3.12.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.9.0+cu126)\n",
      "    Python  3.12.9 (you have 3.12.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: Could not import trl.trainer.alignprop_trainer: Failed to import trl.trainer.alignprop_trainer because of the following error (look up to see its traceback):\n",
      "Failed to import trl.models.modeling_sd_base because of the following error (look up to see its traceback):\n",
      "Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\n",
      "Failed to import diffusers.loaders.ip_adapter because of the following error (look up to see its traceback):\n",
      "JITCallable._set_src() takes 1 positional argument but 2 were given\n",
      "Unsloth: Could not import trl.trainer.ddpo_trainer: Failed to import trl.trainer.ddpo_trainer because of the following error (look up to see its traceback):\n",
      "Failed to import trl.models.modeling_sd_base because of the following error (look up to see its traceback):\n",
      "Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\n",
      "Failed to import diffusers.loaders.ip_adapter because of the following error (look up to see its traceback):\n",
      "JITCallable._set_src() takes 1 positional argument but 2 were given\n",
      "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40229460c5784b7fa0508d6006a7105b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7fcd19809342c9ac3aaa763d2cebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942e8de8078f4e13bb77220510d4daeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6af7ba474b24fb19f4c7551949e91c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05b3e5bda364018ac564616a6fe563d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "936d87b7-5e46-40ba-bb78-23c54318f0d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "### Data Preparation\n",
    "We load the GWU Computer Science course data from `data/combined_courses.csv` and format it into instruction-response pairs for fine-tuning. via functions in the @utils folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd9QyuBE00jo",
    "outputId": "6a44ee5a-2439-47c9-82ba-b88b5090bdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309,
     "referenced_widgets": [
      "46dfb2d44f254922ac7c2f97aabdb993",
      "6ebce7af20824a4d8082d51d07949390",
      "3ef72e6ad37d4e868d2fd13dfeb8d01d",
      "66944c625ef54f4e9dbd08fbd908980d",
      "fa5e05e04ba54ad1ac350c42de403bb6",
      "f8c2f378247f4bcc9610c454c3b228d9",
      "ac91e5f257c646f7af161372ac5798f0",
      "02cbe60c88e44dceaa3b96db45870970",
      "9b19e6987d734849b1580eee3de74e93",
      "305278576e1840c2a7e01071d8963ecb",
      "248b0c63d3704f25910d90138f5598f6",
      "11f8f54ca3954214838b3ef4440063e5",
      "aeec41e5c24a437e9c3bf49bb236a762",
      "52047bd80508450892757d47e4e907a4",
      "19a1ad1bec59417eb1cd1975513aef8f",
      "7f5683120f11435db9fd08e4e12997ec",
      "e3c0f748a750400293e48607a0ab70a8",
      "21ebff98231b440caaca73d0c2b0a1b5",
      "9e0b349db6fa40b48481b58111130d26",
      "95afdb88872f4fa89ec92aa44c81e839",
      "ed93b93f068c4ed8a9a204a1e5cd925b",
      "c1089be9d2ba442695f77652c1aadb53"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "55eac222-c8a1-4fa0-fa87-ee6cea3801db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dfb2d44f254922ac7c2f97aabdb993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f8f54ca3954214838b3ef4440063e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are a helpful assistant providing information about GWU Computer Science and Data Science courses for Spring 2026.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me about CSCI 1012.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The course CSCI 1012: Introduction to Programming with Python is taught by Goldfrank, J. It meets on M 03:45PM - 05:00PM in 1957 E 213. The status is OPEN (CRN: 44900).\n",
      "\n",
      "Description: Introduction to programming a computer using the Python language; variables, types, assignment, conditionals, loops, lists, and program units. (Fall, spring, and summer, Every year)<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# 1. Load the JSONL dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"course_finetune.jsonl\", split=\"train\")\n",
    "\n",
    "# 2. Setup ChatML template (Standard for Llama 3)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\", # Supports zephyr, chatml, mistral, llama, etc.\n",
    "\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# 3. Format the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "### Model Training\n",
    "We configure the `SFTTrainer` and begin fine-tuning the model on our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0de58587bb8d45298d1096d871315668",
      "c82de78464eb421685e3942b27bbb928",
      "577d09d984ed495dae7c248171b56bfc",
      "e71af99a1e7d450e8e949196c799c28e",
      "bd7b7c2a40de444b9b435df65d251df8",
      "9f972b0d441446818dd7f8037a42bb69",
      "0be69b31107e45eb963e7c7a80ae79e3",
      "7e5a02189be4431692a031cf1aacc1ea",
      "5adb85cbb713407ab9ee365c5621df54",
      "87060cd286044c80bf8fde0f10d30589",
      "e817b2bdfc024e2c92bdad74516ef7f1"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "5d786f29-d559-49bf-869e-180c9ea181e6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de58587bb8d45298d1096d871315668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/2828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "079ced81-4e03-47f3-92d0-e3552284d89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
      "7.135 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "95f1640d-92af-4bbd-ca3d-b7bb19111a2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,828 | Num Epochs = 3 | Total steps = 1,062\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1062' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1062/1062 43:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.795800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.833500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.693900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.867800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.825200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.842800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.677900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.700700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.777200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.715600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>0.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>0.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>0.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>0.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>0.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>0.664600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>0.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>0.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>0.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>0.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>0.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>0.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>0.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>0.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>0.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>0.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>0.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>0.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>0.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>0.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.569800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>0.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>0.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>0.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>0.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>0.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>0.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>0.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>0.664600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>0.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>0.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>0.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>0.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>0.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>0.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.756600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>0.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>0.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>0.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>0.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>0.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>0.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>0.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>0.659300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>0.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>0.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>0.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>0.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>0.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>0.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>0.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>0.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>0.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>0.523900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>0.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>0.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>0.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>0.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>0.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>0.558300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>0.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>0.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>0.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>0.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>0.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>0.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>0.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>0.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>0.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>0.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>0.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>0.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>0.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>0.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>0.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>0.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>0.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>0.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>0.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>0.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>0.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>0.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>0.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>0.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>0.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>0.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>0.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>0.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>0.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>0.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>0.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>0.536100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>0.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>0.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>0.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>0.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>0.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>0.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>0.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>0.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>0.546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>0.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>0.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>0.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>0.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>0.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>0.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>0.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>0.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>0.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>0.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>0.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>0.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>0.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>0.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>0.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>0.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>0.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>0.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>0.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>0.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>0.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>0.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>0.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>0.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>0.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>0.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>0.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>0.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>0.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>0.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>0.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>0.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>0.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>0.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>0.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>0.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>0.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>0.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>0.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>0.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>0.728200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>0.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>0.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>0.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>0.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>0.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>0.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>0.415300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>0.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>0.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>0.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>0.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>0.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>0.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>0.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>0.588900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>0.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>0.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>0.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>0.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>0.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>0.557000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>0.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>0.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>0.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>0.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>0.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>0.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>0.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>0.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>0.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>0.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>0.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>0.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>0.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>0.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>0.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>0.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>0.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>0.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>0.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>0.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>0.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>0.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>0.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>0.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>0.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>0.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>0.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>0.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>0.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>0.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>0.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>0.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>0.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>0.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>0.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>0.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>0.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>0.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>0.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>0.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>0.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.588800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>0.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>0.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>0.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>0.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>0.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>0.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "86995bfa-2d17-4494-aa88-1da64f381538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2643.8984 seconds used for training.\n",
      "44.06 minutes used for training.\n",
      "Peak reserved memory = 7.135 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 32.196 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "### Inference\n",
    "We test the model with queries about GWU CS courses to verify it has learned the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "711349f8-8d3e-4017-8dd5-f1452243e775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning (CSCI 6364) is taught by Feng, S.<|reserved_special_token_207|><|reserved_special_token_77|>user<|reserved_special_token_71|>\n",
      "\n",
      "What is the schedule for Machine Learning?<|reserved_special_token_218|><|reserved_special_token_222|>assistant<|reserved_special_token_24|>\n",
      "\n",
      "Machine Learning (CSCI 6364) meets on R 12:45PM - 03:15PM in COR 104.<|reserved_special_token_224|><|reserved_special_token_63|>user<|reserved_special_token_190|>\n",
      "\n",
      "When is the final exam for Machine Learning?<|reserved_special_token_124|><|reserved_special_token_107|>assistant<|reserved_special_token_108|>\n",
      "\n",
      "Machine Learning (CSCI 6364) has a final exam on nan.<|reserved_special_token_52|><|reserved_special_token_102|>user<|reserved_special_token_40|><|reserved_special_token_73|><|reserved_special_token_129|><|reserved_special_token_30|><|reserved_special_token_26|><|reserved_special_token_150|><|reserved_special_token_173|><|reserved_special_token_84|><|reserved_special_token_141|><|reserved_special_token_230|><|reserved_special_token_82|><|reserved_special_token_34|><|reserved_special_token_56|><|reserved_special_token_203|><|reserved_special_token_198|><|python_tag|><|reserved_special_token_146|><|reserved_special_token_53|><|reserved_special_token_70|><|reserved_special_token_234|><|reserved_special_token_69|><|reserved_special_token_213|><|reserved_special_token_105|><|reserved_special_token_189|>Ð°Ñ‚Ð¸ÑÑ<|reserved_special_token_158|><|reserved_special_token_31|><|reserved_special_token_153|>\n",
      "\n",
      "\n",
      "Clean output: Machine Learning (CSCI 6364) is taught by Feng, S.user\n",
      "\n",
      "What is the schedule for Machine Learning?assistant\n",
      "\n",
      "Machine Learning (CSCI 6364) meets on R 12:45PM - 03:15PM in COR 104.user\n",
      "\n",
      "When is the final exam for Machine Learning?assistant\n",
      "\n",
      "Machine Learning (CSCI 6364) has a final exam on nan.userÐ°Ñ‚Ð¸ÑÑ\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant providing information about GWU Computer Science courses for Spring 2026.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who Teaches Machine Learning?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs,\n",
    "    attention_mask = attention_mask,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 128,\n",
    "    temperature = 0.1,  # Lower temperature for more deterministic output\n",
    "    do_sample = True,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    repetition_penalty = 1.1,  # Prevent repetition\n",
    ")\n",
    "\n",
    "# Decode and print clean output\n",
    "output_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"\\n\\nClean output:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "### Save Model\n",
    "We save the fine-tuned LoRA adapters locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "b53a1411-f008-4011-fff1-ee1872292730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved to 'lora_model'.\n"
     ]
    }
   ],
   "source": [
    "# 1. Save LoRA adapters locally (Required for other steps)\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "print(\"LoRA adapters saved to 'lora_model'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "#### Test Saved Model\n",
    "Reload the saved model to confirm it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "6569116f-27a9-472e-cd40-ade56b9c2fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can take CSCI 6908, CSCI 6999, CSCI 8999, CSCI 8901, CSCI 6442, CSCI 6365, CSCI 4907, CSCI 3908, CSCI 3401, CSCI 2460, CSCI 2441W, CSCI 2441, CSCI 2439, CSCI 2113, CSCI 1112, and CSCI 1111.lÃ¡sil<|reserved_special_token_174|>user<|reserved_special_token_77|>\n",
      "\n",
      "When is CSCI 6461 offered?<|reserved_special_token_28|><|reserved_special_token_95|>assistant<|reserved_special_token_102|>\n",
      "\n",
      "CSCI 6461 is scheduled for\n",
      "\n",
      "\n",
      "Clean output: You can take CSCI 6908, CSCI 6999, CSCI 8999, CSCI 8901, CSCI 6442, CSCI 6365, CSCI 4907, CSCI 3908, CSCI 3401, CSCI 2460, CSCI 2441W, CSCI 2441, CSCI 2439, CSCI 2113, CSCI 1112, and CSCI 1111.lÃ¡siluser\n",
      "\n",
      "When is CSCI 6461 offered?assistant\n",
      "\n",
      "CSCI 6461 is scheduled for\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant providing information about GWU Computer Science courses for Spring 2026.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the courses that I can take on tuesdays and wednesdays?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs,\n",
    "    attention_mask = attention_mask,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 128,\n",
    "    temperature = 0.1,  # Lower temperature for more deterministic output\n",
    "    do_sample = True,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    repetition_penalty = 1.1,  # Prevent repetition\n",
    ")\n",
    "\n",
    "# Decode and print clean output\n",
    "output_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"\\n\\nClean output:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Export Options (Float16 / GGUF)\n",
    "Options to export the model for different serving backends (vLLM, llama.cpp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "75ea1a120bea44d7b41d1778c0a8bc17",
      "b66694a5ac6e49e285dace3d1f3d681b",
      "0f413b4d9cf04bd7912219aebf54db75",
      "54935277e2d244d4804f168d8c0d420e",
      "01bd87ebbf954e4c951d6b598c8c40c3",
      "dda9db18f1bc42f395708d1621c910b0",
      "c6cee9e3d87c474ca8336df88663b928",
      "4821b96a5fdd4c74a2b7f26d84bf7c4a",
      "ff07f48a50f54dc79c348e0df822254c",
      "dfc6bcbf07724b719d6dd62c0fb68c4d",
      "58c05370c837459ca0e27385e6cb60db",
      "81cc9045fb63465aab2e822cb45f6fcb",
      "694f745222664e87b0445e00c89289a3",
      "2e3de67a409045f9a408773ec604e879",
      "b6e9ec5f880c4beda20c80542c1bd055",
      "9daa7f66e6214b0cac3e9de153bebd33",
      "617349218db54816b4224a9c8a8ad470",
      "51f85b5cad6748d4927c8c3ca5ebba66",
      "38b7ee6d1d0741a4af24cacadfb6eafb",
      "d8780d85046c4337b9dc6f8c6af4cbb2",
      "4687a8e7f7ac408288d43873a3d909d0",
      "9d5d26f5588b46b8b7c33c0d3e52b448",
      "42bcfb182b3341fdb4ef31029b8e8464",
      "76f2d72598c242ddbc2b04ec0a48ab96",
      "8c0cd148ea504d009ca20c28f3ccb36d",
      "891251ea118a47b481aeb8b682930427",
      "7c5df4ed84cc4076b06665d8975a8aea",
      "09ac839173834c65ba98ff500a87a454",
      "73ac72b9a1be499e8bb70dbb3bf262c9",
      "f08ff8a9cbde4f6da2d38111687cfdbc",
      "042fe86780754cc5bc01a1a1b64c35d8",
      "84fdf3d56c3f4c08a2f3b158dec2a8c5",
      "8839887af99348c29c8e09852bba05c1",
      "3061c54a66af4b9fa880be17c7256965",
      "aac45a4fe0e54bc38dbdbdcc229a7b4f",
      "a60b80a7a9ad4dc6ae007f9cae81ef98",
      "79949e11b7fd4f35b9f3e05995979bd4",
      "6bb5ac2fd4684e3b9adfcbb230b9c978",
      "7fa386f57ad546eea736a853e2632dcd",
      "b1d227e483b2438e982be72524099abe",
      "90a5cf15961a4b7ebd77ae68eaec6c11",
      "8f931047abbc4b4b9561b52494f7b50c",
      "c233ddf3aaba4814af5fe6a0ad8da9b4",
      "9ab8d2324d3c49cdb3667a95590a5ddc",
      "9925b970afe44fefa212198d29d55c9c",
      "3fec468ce1a548bc9426597c1da03b45",
      "b119c436d256434e90524945a655c5a8",
      "cb638db84a5147c5b478be6fbeeb2674",
      "a1583bcf06a64d84891d9ba2f1227f12",
      "4ccf48524e784b09a6481f98b0f6c796",
      "38189c1b3c234dd9bef6c426ac19304c",
      "2f3a93be321b46a6b7d6ded7703068ac",
      "e8b9d37003cb482a89c7f4cffd3401ab",
      "099225b61bdb4957a91fce2bfd20437f",
      "a5c3c2fab26b4bdb83b01888880be38f",
      "a022690309214a078a21f2c87400a971",
      "bc375137cdd84578b06b302490ae8739",
      "65d69afb864b4ef5a6a363e4724e09d3",
      "9ac71d134f0e41eaa257f9672b4ad4aa",
      "0db3df0ecec94721887d5586feea9149",
      "f0160c10d6274dbab615d504671f0ca9",
      "645ab02c4dd147e2aa62736ec9546a44",
      "e8a9e155b87f4a70961ad9ded48e9c29",
      "e7b00a6df6504d959b3baff917167069",
      "26fd7ee5e6554d0a816abfd1988fb811",
      "0e43ea3f644b4e5996f90a0cc120d384",
      "c7addf60951e4da792d5532723d281d6",
      "89172974a6c84e098548962aea7834fa",
      "90ac33efa7b04f6db203c12f70685f52",
      "30cc144d585a4df1a0ddbd29675c7c86",
      "6fb033c665c44345a5dc125b1bc6f118",
      "364e8cb889814f20965efdf8cdbfce0f",
      "6503a75615cd4107b4c56f00eb5875e7",
      "68013a332c614dc5aee87467b9eadd93",
      "a448583cf9d04390a3da134b04a21727",
      "800a4d280b99426abc9adda44dfeaa1e",
      "f0e867e0a242469888e902ef2cb96c38",
      "82ed8f36dfcf4db0a41792e7b47b9d3f",
      "9e49e0abfd804abfa67329cffed62423",
      "4ea1ee8723954712968ce3ec830b19c0",
      "baf1702424ab4af4ac396af63c7937df",
      "c797cff9964c47198a8af7fee700fbf0",
      "4bf260a0b8414fe8a60ff41e3514de29",
      "2d1ab41b630940849166ee3343a51844",
      "d9977ad1331d47d6b19f6b84c34b9ded",
      "9f57e91316c94964b03e6843dcf8a2b2",
      "db5a58b34ffa4459a67c0fe776bdf245",
      "b78d514a4e4f43b7a22804b488166447",
      "130d94f1b86b46fb8da36bf85385ceb6",
      "4f551eb2e62749b190e8123fb7c2ed95",
      "e7a1275314d44d1c815589b0604235a9",
      "778b1d6c0dd840ff8600819d381aa828",
      "b88178fb73cf4a8093402cd122a69f14",
      "17d6290181a54caaa05f0eaf285d49e2",
      "fec8ebcee02f434aa0eb890ff780ac1b",
      "78d543b0301a46f08331a6dced2e49d9",
      "6a8c567b7f554159a0497b48e850a7b2",
      "1162f519d137456d9de27f740b982844",
      "b7662b32708a41168eeef4a3de016d31",
      "739a78a6b20e4976b154bf7fcaf357a7",
      "55e73cc2665e4e9e9d261127b1f8cba8",
      "87b10c18415245e89e88dc670f6df456",
      "e21a8d3a31974b7ba111718e82d6925a",
      "19623eb86e54487e83160596637eb854",
      "158c753f6c9140a29830e3627044b223",
      "5c33c84c30184b1baf2a73ad78f344de",
      "14db138ad3fb4428a9ffbae8e58b7861",
      "c608f2dc0ad645ce9b56bd780b778491",
      "a32dd007e2064df6b59d0dc3da8a1546",
      "60e6341599284714a6083f2c4977296a",
      "3cd881f72f2a40f3ab54f6c491d6390d",
      "2537376157214e17b87d2e513081b6c7",
      "a0663e57e99d4a20b8ec32d77bfca84e",
      "a4ba9ff85eef44bcad645e2a3ec9e729",
      "885b3579bb0841e18564b2db8da36273",
      "ef0dacbb2e8c4f6aa2204d25446a0c71",
      "b9c7328edc5d49d882ca1a164db780bb",
      "a8458e646cfc4ab0a7cd100eb5b094ed",
      "ebb7ddb9d21d425b95635c781db9bf60",
      "e884287d54b04eb4926a9e5049d069fc",
      "005be1366abf4a0baf2d98ab7627781a",
      "52345dbfa7b047bd8b08b00c6b096613",
      "5f4fd4a7421542edaec155c35537b01c",
      "5d85954c158b419fb7d901fc64ac98f9",
      "407aca3d62f24360bdb89d7e5e5eac7a",
      "55c07367d0c844c0a53ffc244889f574",
      "4255cbdf63c242afbfa22589daefd8ad",
      "2acb17dcd0154e3481d02dd9c98bf913",
      "f87162d236984bc5ba85304db606c523",
      "05483817927d46eba0180c30cac73442",
      "f05b0498e050487388f16610a7a7cd4a",
      "1e022efeba504fcdbeca519b40882812",
      "86b050a5494b4328b501a9046bc482d2",
      "27a5a88be04a411aad947e6768b7b596",
      "f4f1a23050a0440eab4b3ae5bcd04afa",
      "913345debb7147be960c174171a14abb",
      "50531ee7460a499d97f8e6c847b52cf2",
      "b27a25744995425b94837912676dfe53",
      "8d80b676967b4a9fb5498b0903e32d5e",
      "d1697f88c3e64a6cabc3c8f8410f0104",
      "d3ece73f274744a6accfd7a5d41828ac",
      "9a92ebe8f31c4bb8abf22743f61df7f5",
      "855738ba5dfd44c7b8a78b227e477256"
     ]
    },
    "id": "iHjt_SMYsd3P",
    "outputId": "a6253189-f9bb-4db4-b2c8-45026f00f668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged_16bit locally...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ea1a120bea44d7b41d1778c0a8bc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/947 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cc9045fb63465aab2e822cb45f6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bcfb182b3341fdb4ef31029b8e8464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:14<00:44, 14.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3061c54a66af4b9fa880be17c7256965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:28<00:28, 14.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9925b970afe44fefa212198d29d55c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:56<00:20, 20.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a022690309214a078a21f2c87400a971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:16<00:00, 19.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:52<00:00, 13.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/merged_model`\n",
      "Ensuring repository itsmepraks/gwcoursesfinetuned exists...\n",
      "Uploading to itsmepraks/gwcoursesfinetuned...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7addf60951e4da792d5532723d281d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ed8f36dfcf4db0a41792e7b47b9d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130d94f1b86b46fb8da36bf85385ceb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...rged_model/tokenizer.json:   2%|1         |  295kB / 17.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739a78a6b20e4976b154bf7fcaf357a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0002-of-00004.safetensors:   0%|          |  612kB / 5.00GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd881f72f2a40f3ab54f6c491d6390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0003-of-00004.safetensors:   0%|          |  613kB / 4.92GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52345dbfa7b047bd8b08b00c6b096613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0004-of-00004.safetensors:   3%|2         | 33.5MB / 1.17GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b050a5494b4328b501a9046bc482d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0001-of-00004.safetensors:   1%|          | 25.1MB / 4.98GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/itsmepraks/gwcoursesfinetuned/commit/ecccfb99c8624fe136e2eb7bf8cfc3e57bbb6cff', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ecccfb99c8624fe136e2eb7bf8cfc3e57bbb6cff', pr_url=None, repo_url=RepoUrl('https://huggingface.co/itsmepraks/gwcoursesfinetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='itsmepraks/gwcoursesfinetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Export to Hugging Face (Merged Model)\n",
    "# Options: \"merged_16bit\" (High precision), \"merged_4bit\" (Smaller), \"lora\" (Adapters only)\n",
    "save_method = \"merged_16bit\"\n",
    "\n",
    "# First save the model locally\n",
    "print(f\"Saving {save_method} locally...\")\n",
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=save_method)\n",
    "\n",
    "# Then push to Hugging Face using HfApi\n",
    "from huggingface_hub import HfApi\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# robustly retrieve token\n",
    "try:\n",
    "    token = userdata.get(\"HF_TOKEN\")\n",
    "except Exception:\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    # Stop execution here with a clear message if no token is found\n",
    "    raise ValueError(\"HF_TOKEN not found! Please add 'HF_TOKEN' to the Colab Secrets (key icon on left) and toggle 'Notebook access' on.\")\n",
    "\n",
    "api = HfApi(token=token)\n",
    "\n",
    "repo_id = \"itsmepraks/gwcoursesfinetuned\"\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "print(f\"Ensuring repository {repo_id} exists...\")\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "print(f\"Uploading to {repo_id}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=\"merged_model\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ROpZYKdKU-F",
    "outputId": "ab9c7270-69b8-4b42-a00a-58e965a58fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 1062 epochs\n",
      "âœ“ Training time: 44.06 minutes\n",
      "âœ“ Final loss: 0.4559\n",
      "âœ“ Saved to: standard_training_metrics.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'approaches': {'standard': {'name': 'Standard Fine-tuning',\n",
       "   'epochs': [{'epoch': 0, 'train_loss': 4.8612, 'learning_rate': 0.0},\n",
       "    {'epoch': 0, 'train_loss': 4.2469, 'learning_rate': 4e-05},\n",
       "    {'epoch': 0, 'train_loss': 4.4241, 'learning_rate': 8e-05},\n",
       "    {'epoch': 0, 'train_loss': 4.1916, 'learning_rate': 0.00012},\n",
       "    {'epoch': 0, 'train_loss': 3.9006, 'learning_rate': 0.00016},\n",
       "    {'epoch': 0, 'train_loss': 3.9619, 'learning_rate': 0.0002},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 3.5979,\n",
       "     'learning_rate': 0.00019981078524124884},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 3.2079,\n",
       "     'learning_rate': 0.00019962157048249765},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 2.6276,\n",
       "     'learning_rate': 0.00019943235572374645},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 2.2404,\n",
       "     'learning_rate': 0.00019924314096499528},\n",
       "    {'epoch': 0, 'train_loss': 1.9319, 'learning_rate': 0.0001990539262062441},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.8762,\n",
       "     'learning_rate': 0.00019886471144749292},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.9011,\n",
       "     'learning_rate': 0.00019867549668874172},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.6885,\n",
       "     'learning_rate': 0.00019848628192999055},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.5349,\n",
       "     'learning_rate': 0.00019829706717123936},\n",
       "    {'epoch': 0, 'train_loss': 1.4434, 'learning_rate': 0.0001981078524124882},\n",
       "    {'epoch': 0, 'train_loss': 1.4522, 'learning_rate': 0.000197918637653737},\n",
       "    {'epoch': 0, 'train_loss': 1.386, 'learning_rate': 0.00019772942289498582},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.2972,\n",
       "     'learning_rate': 0.00019754020813623463},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.3768,\n",
       "     'learning_rate': 0.00019735099337748346},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.3117,\n",
       "     'learning_rate': 0.00019716177861873227},\n",
       "    {'epoch': 0, 'train_loss': 1.3851, 'learning_rate': 0.0001969725638599811},\n",
       "    {'epoch': 0, 'train_loss': 1.4277, 'learning_rate': 0.0001967833491012299},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.2423,\n",
       "     'learning_rate': 0.00019659413434247873},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.1067,\n",
       "     'learning_rate': 0.00019640491958372754},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.2052,\n",
       "     'learning_rate': 0.00019621570482497634},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.2229,\n",
       "     'learning_rate': 0.00019602649006622517},\n",
       "    {'epoch': 0, 'train_loss': 1.3332, 'learning_rate': 0.000195837275307474},\n",
       "    {'epoch': 0, 'train_loss': 1.2154, 'learning_rate': 0.0001956480605487228},\n",
       "    {'epoch': 0, 'train_loss': 1.147, 'learning_rate': 0.0001954588457899716},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.1578,\n",
       "     'learning_rate': 0.00019526963103122044},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.1511,\n",
       "     'learning_rate': 0.00019508041627246928},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9101,\n",
       "     'learning_rate': 0.00019489120151371808},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.1713,\n",
       "     'learning_rate': 0.00019470198675496689},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.1176,\n",
       "     'learning_rate': 0.00019451277199621572},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0407,\n",
       "     'learning_rate': 0.00019432355723746455},\n",
       "    {'epoch': 0, 'train_loss': 1.146, 'learning_rate': 0.00019413434247871333},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0422,\n",
       "     'learning_rate': 0.00019394512771996216},\n",
       "    {'epoch': 0, 'train_loss': 1.0216, 'learning_rate': 0.000193755912961211},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9827,\n",
       "     'learning_rate': 0.00019356669820245982},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9919,\n",
       "     'learning_rate': 0.00019337748344370862},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0361,\n",
       "     'learning_rate': 0.00019318826868495743},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9423,\n",
       "     'learning_rate': 0.00019299905392620626},\n",
       "    {'epoch': 0, 'train_loss': 0.936, 'learning_rate': 0.00019280983916745506},\n",
       "    {'epoch': 0, 'train_loss': 0.9979, 'learning_rate': 0.0001926206244087039},\n",
       "    {'epoch': 0, 'train_loss': 0.9763, 'learning_rate': 0.0001924314096499527},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0564,\n",
       "     'learning_rate': 0.00019224219489120153},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8935,\n",
       "     'learning_rate': 0.00019205298013245034},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9153,\n",
       "     'learning_rate': 0.00019186376537369917},\n",
       "    {'epoch': 0, 'train_loss': 1.028, 'learning_rate': 0.00019167455061494797},\n",
       "    {'epoch': 0, 'train_loss': 0.9802, 'learning_rate': 0.0001914853358561968},\n",
       "    {'epoch': 0, 'train_loss': 0.9331, 'learning_rate': 0.0001912961210974456},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8742,\n",
       "     'learning_rate': 0.00019110690633869444},\n",
       "    {'epoch': 0, 'train_loss': 0.82, 'learning_rate': 0.00019091769157994324},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7958,\n",
       "     'learning_rate': 0.00019072847682119205},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8219,\n",
       "     'learning_rate': 0.00019053926206244088},\n",
       "    {'epoch': 0, 'train_loss': 0.9556, 'learning_rate': 0.0001903500473036897},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9285,\n",
       "     'learning_rate': 0.00019016083254493852},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0649,\n",
       "     'learning_rate': 0.00018997161778618732},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7634,\n",
       "     'learning_rate': 0.00018978240302743615},\n",
       "    {'epoch': 0, 'train_loss': 0.86, 'learning_rate': 0.00018959318826868498},\n",
       "    {'epoch': 0, 'train_loss': 0.9722, 'learning_rate': 0.0001894039735099338},\n",
       "    {'epoch': 0, 'train_loss': 0.934, 'learning_rate': 0.0001892147587511826},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8437,\n",
       "     'learning_rate': 0.00018902554399243142},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8058,\n",
       "     'learning_rate': 0.00018883632923368026},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8561,\n",
       "     'learning_rate': 0.00018864711447492903},\n",
       "    {'epoch': 0, 'train_loss': 0.715, 'learning_rate': 0.00018845789971617786},\n",
       "    {'epoch': 0, 'train_loss': 1.0196, 'learning_rate': 0.0001882686849574267},\n",
       "    {'epoch': 0, 'train_loss': 0.776, 'learning_rate': 0.0001880794701986755},\n",
       "    {'epoch': 0, 'train_loss': 0.8778, 'learning_rate': 0.0001878902554399243},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8987,\n",
       "     'learning_rate': 0.00018770104068117314},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8046,\n",
       "     'learning_rate': 0.00018751182592242197},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7557,\n",
       "     'learning_rate': 0.00018732261116367077},\n",
       "    {'epoch': 0, 'train_loss': 0.8175, 'learning_rate': 0.0001871333964049196},\n",
       "    {'epoch': 0, 'train_loss': 0.9292, 'learning_rate': 0.0001869441816461684},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9427,\n",
       "     'learning_rate': 0.00018675496688741724},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9007,\n",
       "     'learning_rate': 0.00018656575212866604},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8847,\n",
       "     'learning_rate': 0.00018637653736991487},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7237,\n",
       "     'learning_rate': 0.00018618732261116368},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8611,\n",
       "     'learning_rate': 0.00018599810785241248},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0511,\n",
       "     'learning_rate': 0.00018580889309366132},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8917,\n",
       "     'learning_rate': 0.00018561967833491015},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9783,\n",
       "     'learning_rate': 0.00018543046357615895},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0134,\n",
       "     'learning_rate': 0.00018524124881740776},\n",
       "    {'epoch': 0, 'train_loss': 0.8425, 'learning_rate': 0.0001850520340586566},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7416,\n",
       "     'learning_rate': 0.00018486281929990542},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7918,\n",
       "     'learning_rate': 0.00018467360454115422},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8335,\n",
       "     'learning_rate': 0.00018448438978240303},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8866,\n",
       "     'learning_rate': 0.00018429517502365186},\n",
       "    {'epoch': 0, 'train_loss': 0.7764, 'learning_rate': 0.0001841059602649007},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8893,\n",
       "     'learning_rate': 0.00018391674550614947},\n",
       "    {'epoch': 0, 'train_loss': 0.7499, 'learning_rate': 0.0001837275307473983},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9053,\n",
       "     'learning_rate': 0.00018353831598864713},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7696,\n",
       "     'learning_rate': 0.00018334910122989593},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7637,\n",
       "     'learning_rate': 0.00018315988647114474},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9046,\n",
       "     'learning_rate': 0.00018297067171239357},\n",
       "    {'epoch': 0, 'train_loss': 0.8608, 'learning_rate': 0.0001827814569536424},\n",
       "    {'epoch': 0, 'train_loss': 0.9398, 'learning_rate': 0.0001825922421948912},\n",
       "    {'epoch': 0, 'train_loss': 0.7445, 'learning_rate': 0.00018240302743614},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7757,\n",
       "     'learning_rate': 0.00018221381267738884},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 1.0931,\n",
       "     'learning_rate': 0.00018202459791863767},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8332,\n",
       "     'learning_rate': 0.00018183538315988648},\n",
       "    {'epoch': 0, 'train_loss': 0.6939, 'learning_rate': 0.0001816461684011353},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9098,\n",
       "     'learning_rate': 0.00018145695364238411},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7145,\n",
       "     'learning_rate': 0.00018126773888363292},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.9118,\n",
       "     'learning_rate': 0.00018107852412488175},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8102,\n",
       "     'learning_rate': 0.00018088930936613058},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7875,\n",
       "     'learning_rate': 0.00018070009460737939},\n",
       "    {'epoch': 0, 'train_loss': 0.8782, 'learning_rate': 0.0001805108798486282},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7776,\n",
       "     'learning_rate': 0.00018032166508987702},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7421,\n",
       "     'learning_rate': 0.00018013245033112585},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8323,\n",
       "     'learning_rate': 0.00017994323557237466},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8043,\n",
       "     'learning_rate': 0.00017975402081362346},\n",
       "    {'epoch': 0, 'train_loss': 0.8678, 'learning_rate': 0.0001795648060548723},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7417,\n",
       "     'learning_rate': 0.00017937559129612113},\n",
       "    {'epoch': 0, 'train_loss': 0.8977, 'learning_rate': 0.0001791863765373699},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7655,\n",
       "     'learning_rate': 0.00017899716177861873},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7791,\n",
       "     'learning_rate': 0.00017880794701986757},\n",
       "    {'epoch': 0, 'train_loss': 0.83, 'learning_rate': 0.0001786187322611164},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8358,\n",
       "     'learning_rate': 0.00017842951750236517},\n",
       "    {'epoch': 0, 'train_loss': 0.7822, 'learning_rate': 0.000178240302743614},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6073,\n",
       "     'learning_rate': 0.00017805108798486284},\n",
       "    {'epoch': 0, 'train_loss': 0.752, 'learning_rate': 0.00017786187322611164},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7148,\n",
       "     'learning_rate': 0.00017767265846736045},\n",
       "    {'epoch': 0, 'train_loss': 0.7, 'learning_rate': 0.00017748344370860928},\n",
       "    {'epoch': 0, 'train_loss': 0.8431, 'learning_rate': 0.0001772942289498581},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8247,\n",
       "     'learning_rate': 0.00017710501419110691},\n",
       "    {'epoch': 0, 'train_loss': 0.726, 'learning_rate': 0.00017691579943235572},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7357,\n",
       "     'learning_rate': 0.00017672658467360455},\n",
       "    {'epoch': 0, 'train_loss': 0.795, 'learning_rate': 0.00017653736991485338},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7666,\n",
       "     'learning_rate': 0.00017634815515610219},\n",
       "    {'epoch': 0, 'train_loss': 0.7956, 'learning_rate': 0.000176158940397351},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5974,\n",
       "     'learning_rate': 0.00017596972563859982},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6733,\n",
       "     'learning_rate': 0.00017578051087984863},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7617,\n",
       "     'learning_rate': 0.00017559129612109746},\n",
       "    {'epoch': 0, 'train_loss': 0.8796, 'learning_rate': 0.0001754020813623463},\n",
       "    {'epoch': 0, 'train_loss': 0.7303, 'learning_rate': 0.0001752128666035951},\n",
       "    {'epoch': 0, 'train_loss': 0.8287, 'learning_rate': 0.0001750236518448439},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6959,\n",
       "     'learning_rate': 0.00017483443708609273},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6938,\n",
       "     'learning_rate': 0.00017464522232734156},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7998,\n",
       "     'learning_rate': 0.00017445600756859037},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7106,\n",
       "     'learning_rate': 0.00017426679280983917},\n",
       "    {'epoch': 0, 'train_loss': 0.6741, 'learning_rate': 0.000174077578051088},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7638,\n",
       "     'learning_rate': 0.00017388836329233683},\n",
       "    {'epoch': 0, 'train_loss': 0.7667, 'learning_rate': 0.0001736991485335856},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6177,\n",
       "     'learning_rate': 0.00017350993377483444},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7063,\n",
       "     'learning_rate': 0.00017332071901608327},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6705,\n",
       "     'learning_rate': 0.00017313150425733208},\n",
       "    {'epoch': 0, 'train_loss': 0.746, 'learning_rate': 0.00017294228949858088},\n",
       "    {'epoch': 0, 'train_loss': 0.6908, 'learning_rate': 0.0001727530747398297},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7267,\n",
       "     'learning_rate': 0.00017256385998107854},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7737,\n",
       "     'learning_rate': 0.00017237464522232735},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6958,\n",
       "     'learning_rate': 0.00017218543046357615},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6537,\n",
       "     'learning_rate': 0.00017199621570482498},\n",
       "    {'epoch': 0, 'train_loss': 0.691, 'learning_rate': 0.00017180700094607382},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7375,\n",
       "     'learning_rate': 0.00017161778618732262},\n",
       "    {'epoch': 0, 'train_loss': 0.806, 'learning_rate': 0.00017142857142857143},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6809,\n",
       "     'learning_rate': 0.00017123935666982026},\n",
       "    {'epoch': 0, 'train_loss': 0.853, 'learning_rate': 0.00017105014191106906},\n",
       "    {'epoch': 0, 'train_loss': 0.6663, 'learning_rate': 0.0001708609271523179},\n",
       "    {'epoch': 0, 'train_loss': 0.7123, 'learning_rate': 0.0001706717123935667},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8596,\n",
       "     'learning_rate': 0.00017048249763481553},\n",
       "    {'epoch': 0, 'train_loss': 0.577, 'learning_rate': 0.00017029328287606433},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6667,\n",
       "     'learning_rate': 0.00017010406811731316},\n",
       "    {'epoch': 0, 'train_loss': 0.846, 'learning_rate': 0.00016991485335856197},\n",
       "    {'epoch': 0, 'train_loss': 0.7126, 'learning_rate': 0.0001697256385998108},\n",
       "    {'epoch': 0, 'train_loss': 0.8032, 'learning_rate': 0.0001695364238410596},\n",
       "    {'epoch': 0, 'train_loss': 0.634, 'learning_rate': 0.00016934720908230844},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7185,\n",
       "     'learning_rate': 0.00016915799432355727},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7383,\n",
       "     'learning_rate': 0.00016896877956480604},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7205,\n",
       "     'learning_rate': 0.00016877956480605488},\n",
       "    {'epoch': 0, 'train_loss': 0.6619, 'learning_rate': 0.0001685903500473037},\n",
       "    {'epoch': 0, 'train_loss': 0.6476, 'learning_rate': 0.0001684011352885525},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6251,\n",
       "     'learning_rate': 0.00016821192052980132},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7373,\n",
       "     'learning_rate': 0.00016802270577105015},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6911,\n",
       "     'learning_rate': 0.00016783349101229898},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6598,\n",
       "     'learning_rate': 0.00016764427625354778},\n",
       "    {'epoch': 0, 'train_loss': 0.5543, 'learning_rate': 0.0001674550614947966},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6843,\n",
       "     'learning_rate': 0.00016726584673604542},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6798,\n",
       "     'learning_rate': 0.00016707663197729425},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6405,\n",
       "     'learning_rate': 0.00016688741721854306},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6814,\n",
       "     'learning_rate': 0.00016669820245979186},\n",
       "    {'epoch': 0, 'train_loss': 0.7392, 'learning_rate': 0.0001665089877010407},\n",
       "    {'epoch': 0, 'train_loss': 0.6187, 'learning_rate': 0.0001663197729422895},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6141,\n",
       "     'learning_rate': 0.00016613055818353833},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6911,\n",
       "     'learning_rate': 0.00016594134342478713},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6914,\n",
       "     'learning_rate': 0.00016575212866603596},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7263,\n",
       "     'learning_rate': 0.00016556291390728477},\n",
       "    {'epoch': 0, 'train_loss': 0.6615, 'learning_rate': 0.0001653736991485336},\n",
       "    {'epoch': 0, 'train_loss': 0.5169, 'learning_rate': 0.0001651844843897824},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6578,\n",
       "     'learning_rate': 0.00016499526963103124},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8252,\n",
       "     'learning_rate': 0.00016480605487228004},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7303,\n",
       "     'learning_rate': 0.00016461684011352887},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6729,\n",
       "     'learning_rate': 0.00016442762535477768},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7473,\n",
       "     'learning_rate': 0.00016423841059602648},\n",
       "    {'epoch': 0, 'train_loss': 0.6608, 'learning_rate': 0.0001640491958372753},\n",
       "    {'epoch': 0, 'train_loss': 0.739, 'learning_rate': 0.00016385998107852414},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6258,\n",
       "     'learning_rate': 0.00016367076631977295},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7788,\n",
       "     'learning_rate': 0.00016348155156102175},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7196,\n",
       "     'learning_rate': 0.00016329233680227058},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7227,\n",
       "     'learning_rate': 0.00016310312204351941},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8779,\n",
       "     'learning_rate': 0.00016291390728476822},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7115,\n",
       "     'learning_rate': 0.00016272469252601702},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7335,\n",
       "     'learning_rate': 0.00016253547776726586},\n",
       "    {'epoch': 0, 'train_loss': 0.7584, 'learning_rate': 0.0001623462630085147},\n",
       "    {'epoch': 0, 'train_loss': 0.7036, 'learning_rate': 0.0001621570482497635},\n",
       "    {'epoch': 0, 'train_loss': 0.6712, 'learning_rate': 0.0001619678334910123},\n",
       "    {'epoch': 0, 'train_loss': 0.642, 'learning_rate': 0.00016177861873226113},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7804,\n",
       "     'learning_rate': 0.00016158940397350996},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7107,\n",
       "     'learning_rate': 0.00016140018921475876},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6618,\n",
       "     'learning_rate': 0.00016121097445600757},\n",
       "    {'epoch': 0, 'train_loss': 0.8313, 'learning_rate': 0.0001610217596972564},\n",
       "    {'epoch': 0, 'train_loss': 0.6686, 'learning_rate': 0.0001608325449385052},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6375,\n",
       "     'learning_rate': 0.00016064333017975403},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6638,\n",
       "     'learning_rate': 0.00016045411542100284},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7354,\n",
       "     'learning_rate': 0.00016026490066225167},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6813,\n",
       "     'learning_rate': 0.00016007568590350048},\n",
       "    {'epoch': 0, 'train_loss': 0.6251, 'learning_rate': 0.0001598864711447493},\n",
       "    {'epoch': 0, 'train_loss': 0.6655, 'learning_rate': 0.0001596972563859981},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7204,\n",
       "     'learning_rate': 0.00015950804162724694},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6337,\n",
       "     'learning_rate': 0.00015931882686849575},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5878,\n",
       "     'learning_rate': 0.00015912961210974458},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6242,\n",
       "     'learning_rate': 0.00015894039735099338},\n",
       "    {'epoch': 0, 'train_loss': 0.7652, 'learning_rate': 0.0001587511825922422},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8428,\n",
       "     'learning_rate': 0.00015856196783349102},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7334,\n",
       "     'learning_rate': 0.00015837275307473985},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6204,\n",
       "     'learning_rate': 0.00015818353831598865},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6541,\n",
       "     'learning_rate': 0.00015799432355723746},\n",
       "    {'epoch': 0, 'train_loss': 0.6219, 'learning_rate': 0.0001578051087984863},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5641,\n",
       "     'learning_rate': 0.00015761589403973512},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7245,\n",
       "     'learning_rate': 0.00015742667928098393},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6799,\n",
       "     'learning_rate': 0.00015723746452223273},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7635,\n",
       "     'learning_rate': 0.00015704824976348156},\n",
       "    {'epoch': 0, 'train_loss': 0.6889, 'learning_rate': 0.0001568590350047304},\n",
       "    {'epoch': 0, 'train_loss': 0.7678, 'learning_rate': 0.0001566698202459792},\n",
       "    {'epoch': 0, 'train_loss': 0.7554, 'learning_rate': 0.000156480605487228},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.8596,\n",
       "     'learning_rate': 0.00015629139072847683},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6623,\n",
       "     'learning_rate': 0.00015610217596972564},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6892,\n",
       "     'learning_rate': 0.00015591296121097447},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6335,\n",
       "     'learning_rate': 0.00015572374645222327},\n",
       "    {'epoch': 0, 'train_loss': 0.666, 'learning_rate': 0.0001555345316934721},\n",
       "    {'epoch': 0, 'train_loss': 0.6817, 'learning_rate': 0.0001553453169347209},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5673,\n",
       "     'learning_rate': 0.00015515610217596974},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6588,\n",
       "     'learning_rate': 0.00015496688741721855},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6164,\n",
       "     'learning_rate': 0.00015477767265846738},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5927,\n",
       "     'learning_rate': 0.00015458845789971618},\n",
       "    {'epoch': 0, 'train_loss': 0.665, 'learning_rate': 0.00015439924314096501},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7037,\n",
       "     'learning_rate': 0.00015421002838221382},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6574,\n",
       "     'learning_rate': 0.00015402081362346262},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6433,\n",
       "     'learning_rate': 0.00015383159886471145},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7121,\n",
       "     'learning_rate': 0.00015364238410596029},\n",
       "    {'epoch': 0, 'train_loss': 0.5657, 'learning_rate': 0.0001534531693472091},\n",
       "    {'epoch': 0, 'train_loss': 0.7276, 'learning_rate': 0.0001532639545884579},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6311,\n",
       "     'learning_rate': 0.00015307473982970673},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7312,\n",
       "     'learning_rate': 0.00015288552507095556},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7652,\n",
       "     'learning_rate': 0.00015269631031220436},\n",
       "    {'epoch': 0, 'train_loss': 0.768, 'learning_rate': 0.00015250709555345317},\n",
       "    {'epoch': 0, 'train_loss': 0.654, 'learning_rate': 0.000152317880794702},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6994,\n",
       "     'learning_rate': 0.00015212866603595083},\n",
       "    {'epoch': 0, 'train_loss': 0.6653, 'learning_rate': 0.0001519394512771996},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6426,\n",
       "     'learning_rate': 0.00015175023651844844},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7641,\n",
       "     'learning_rate': 0.00015156102175969727},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6277,\n",
       "     'learning_rate': 0.00015137180700094607},\n",
       "    {'epoch': 0, 'train_loss': 0.6797, 'learning_rate': 0.0001511825922421949},\n",
       "    {'epoch': 0, 'train_loss': 0.6705, 'learning_rate': 0.0001509933774834437},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7476,\n",
       "     'learning_rate': 0.00015080416272469254},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6115,\n",
       "     'learning_rate': 0.00015061494796594135},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6645,\n",
       "     'learning_rate': 0.00015042573320719018},\n",
       "    {'epoch': 0, 'train_loss': 0.694, 'learning_rate': 0.00015023651844843898},\n",
       "    {'epoch': 0, 'train_loss': 0.6103, 'learning_rate': 0.0001500473036896878},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7999,\n",
       "     'learning_rate': 0.00014985808893093662},\n",
       "    {'epoch': 0, 'train_loss': 0.568, 'learning_rate': 0.00014966887417218545},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7606,\n",
       "     'learning_rate': 0.00014947965941343425},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7536,\n",
       "     'learning_rate': 0.00014929044465468306},\n",
       "    {'epoch': 0, 'train_loss': 0.707, 'learning_rate': 0.0001491012298959319},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6779,\n",
       "     'learning_rate': 0.00014891201513718072},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6982,\n",
       "     'learning_rate': 0.00014872280037842952},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6954,\n",
       "     'learning_rate': 0.00014853358561967833},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7623,\n",
       "     'learning_rate': 0.00014834437086092716},\n",
       "    {'epoch': 0, 'train_loss': 0.7524, 'learning_rate': 0.000148155156102176},\n",
       "    {'epoch': 0, 'train_loss': 0.7035, 'learning_rate': 0.0001479659413434248},\n",
       "    {'epoch': 0, 'train_loss': 0.7653, 'learning_rate': 0.0001477767265846736},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6892,\n",
       "     'learning_rate': 0.00014758751182592243},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6554,\n",
       "     'learning_rate': 0.00014739829706717126},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7454,\n",
       "     'learning_rate': 0.00014720908230842004},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5703,\n",
       "     'learning_rate': 0.00014701986754966887},\n",
       "    {'epoch': 0, 'train_loss': 0.6448, 'learning_rate': 0.0001468306527909177},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7007,\n",
       "     'learning_rate': 0.00014664143803216654},\n",
       "    {'epoch': 0, 'train_loss': 0.6321, 'learning_rate': 0.0001464522232734153},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6043,\n",
       "     'learning_rate': 0.00014626300851466414},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5717,\n",
       "     'learning_rate': 0.00014607379375591298},\n",
       "    {'epoch': 0, 'train_loss': 0.603, 'learning_rate': 0.00014588457899716178},\n",
       "    {'epoch': 0, 'train_loss': 0.682, 'learning_rate': 0.00014569536423841059},\n",
       "    {'epoch': 0, 'train_loss': 0.716, 'learning_rate': 0.00014550614947965942},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6444,\n",
       "     'learning_rate': 0.00014531693472090825},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7159,\n",
       "     'learning_rate': 0.00014512771996215705},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5584,\n",
       "     'learning_rate': 0.00014493850520340588},\n",
       "    {'epoch': 0, 'train_loss': 0.6115, 'learning_rate': 0.0001447492904446547},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6346,\n",
       "     'learning_rate': 0.00014456007568590352},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6497,\n",
       "     'learning_rate': 0.00014437086092715232},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6104,\n",
       "     'learning_rate': 0.00014418164616840116},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6597,\n",
       "     'learning_rate': 0.00014399243140964996},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.4974,\n",
       "     'learning_rate': 0.00014380321665089876},\n",
       "    {'epoch': 0, 'train_loss': 0.5701, 'learning_rate': 0.0001436140018921476},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7167,\n",
       "     'learning_rate': 0.00014342478713339643},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5281,\n",
       "     'learning_rate': 0.00014323557237464523},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6268,\n",
       "     'learning_rate': 0.00014304635761589404},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6455,\n",
       "     'learning_rate': 0.00014285714285714287},\n",
       "    {'epoch': 0, 'train_loss': 0.6768, 'learning_rate': 0.0001426679280983917},\n",
       "    {'epoch': 0, 'train_loss': 0.612, 'learning_rate': 0.0001424787133396405},\n",
       "    {'epoch': 0, 'train_loss': 0.5918, 'learning_rate': 0.0001422894985808893},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6448,\n",
       "     'learning_rate': 0.00014210028382213814},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6712,\n",
       "     'learning_rate': 0.00014191106906338697},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6504,\n",
       "     'learning_rate': 0.00014172185430463575},\n",
       "    {'epoch': 0, 'train_loss': 0.628, 'learning_rate': 0.00014153263954588458},\n",
       "    {'epoch': 0, 'train_loss': 0.6071, 'learning_rate': 0.0001413434247871334},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6715,\n",
       "     'learning_rate': 0.00014115421002838222},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6085,\n",
       "     'learning_rate': 0.00014096499526963102},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5644,\n",
       "     'learning_rate': 0.00014077578051087985},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7219,\n",
       "     'learning_rate': 0.00014058656575212868},\n",
       "    {'epoch': 0, 'train_loss': 0.6136, 'learning_rate': 0.0001403973509933775},\n",
       "    {'epoch': 0, 'train_loss': 0.747, 'learning_rate': 0.0001402081362346263},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6469,\n",
       "     'learning_rate': 0.00014001892147587512},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6168,\n",
       "     'learning_rate': 0.00013982970671712396},\n",
       "    {'epoch': 0, 'train_loss': 0.606, 'learning_rate': 0.00013964049195837276},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7068,\n",
       "     'learning_rate': 0.00013945127719962156},\n",
       "    {'epoch': 0, 'train_loss': 0.7772, 'learning_rate': 0.0001392620624408704},\n",
       "    {'epoch': 0, 'train_loss': 0.6247, 'learning_rate': 0.0001390728476821192},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6322,\n",
       "     'learning_rate': 0.00013888363292336803},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6891,\n",
       "     'learning_rate': 0.00013869441816461686},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7156,\n",
       "     'learning_rate': 0.00013850520340586567},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6851,\n",
       "     'learning_rate': 0.00013831598864711447},\n",
       "    {'epoch': 0, 'train_loss': 0.7353, 'learning_rate': 0.0001381267738883633},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6795,\n",
       "     'learning_rate': 0.00013793755912961213},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6142,\n",
       "     'learning_rate': 0.00013774834437086094},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5855,\n",
       "     'learning_rate': 0.00013755912961210974},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7382,\n",
       "     'learning_rate': 0.00013736991485335857},\n",
       "    {'epoch': 0, 'train_loss': 0.6624, 'learning_rate': 0.0001371807000946074},\n",
       "    {'epoch': 0, 'train_loss': 0.781, 'learning_rate': 0.00013699148533585618},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7097,\n",
       "     'learning_rate': 0.00013680227057710502},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6126,\n",
       "     'learning_rate': 0.00013661305581835385},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.7025,\n",
       "     'learning_rate': 0.00013642384105960265},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6967,\n",
       "     'learning_rate': 0.00013623462630085146},\n",
       "    {'epoch': 0, 'train_loss': 0.6324, 'learning_rate': 0.0001360454115421003},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5791,\n",
       "     'learning_rate': 0.00013585619678334912},\n",
       "    {'epoch': 0, 'train_loss': 0.604, 'learning_rate': 0.00013566698202459792},\n",
       "    {'epoch': 0, 'train_loss': 0.702, 'learning_rate': 0.00013547776726584673},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6768,\n",
       "     'learning_rate': 0.00013528855250709556},\n",
       "    {'epoch': 0, 'train_loss': 0.689, 'learning_rate': 0.0001350993377483444},\n",
       "    {'epoch': 0, 'train_loss': 0.6888, 'learning_rate': 0.0001349101229895932},\n",
       "    {'epoch': 0, 'train_loss': 0.6315, 'learning_rate': 0.000134720908230842},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.6152,\n",
       "     'learning_rate': 0.00013453169347209083},\n",
       "    {'epoch': 0,\n",
       "     'train_loss': 0.5806,\n",
       "     'learning_rate': 0.00013434247871333963},\n",
       "    {'epoch': 1, 'train_loss': 0.536, 'learning_rate': 0.00013415326395458847},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5977,\n",
       "     'learning_rate': 0.00013396404919583727},\n",
       "    {'epoch': 1, 'train_loss': 0.6176, 'learning_rate': 0.0001337748344370861},\n",
       "    {'epoch': 1, 'train_loss': 0.6762, 'learning_rate': 0.0001335856196783349},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5438,\n",
       "     'learning_rate': 0.00013339640491958374},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6343,\n",
       "     'learning_rate': 0.00013320719016083257},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6837,\n",
       "     'learning_rate': 0.00013301797540208137},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6633,\n",
       "     'learning_rate': 0.00013282876064333018},\n",
       "    {'epoch': 1, 'train_loss': 0.5712, 'learning_rate': 0.000132639545884579},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6244,\n",
       "     'learning_rate': 0.00013245033112582784},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6796,\n",
       "     'learning_rate': 0.00013226111636707662},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6234,\n",
       "     'learning_rate': 0.00013207190160832545},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6284,\n",
       "     'learning_rate': 0.00013188268684957428},\n",
       "    {'epoch': 1, 'train_loss': 0.6289, 'learning_rate': 0.0001316934720908231},\n",
       "    {'epoch': 1, 'train_loss': 0.5181, 'learning_rate': 0.0001315042573320719},\n",
       "    {'epoch': 1, 'train_loss': 0.695, 'learning_rate': 0.00013131504257332072},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6214,\n",
       "     'learning_rate': 0.00013112582781456955},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6841,\n",
       "     'learning_rate': 0.00013093661305581836},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6467,\n",
       "     'learning_rate': 0.00013074739829706716},\n",
       "    {'epoch': 1, 'train_loss': 0.5839, 'learning_rate': 0.000130558183538316},\n",
       "    {'epoch': 1, 'train_loss': 0.635, 'learning_rate': 0.00013036896877956483},\n",
       "    {'epoch': 1, 'train_loss': 0.615, 'learning_rate': 0.00013017975402081363},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6373,\n",
       "     'learning_rate': 0.00012999053926206243},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6793,\n",
       "     'learning_rate': 0.00012980132450331127},\n",
       "    {'epoch': 1, 'train_loss': 0.621, 'learning_rate': 0.0001296121097445601},\n",
       "    {'epoch': 1, 'train_loss': 0.5517, 'learning_rate': 0.0001294228949858089},\n",
       "    {'epoch': 1, 'train_loss': 0.5449, 'learning_rate': 0.0001292336802270577},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5335,\n",
       "     'learning_rate': 0.00012904446546830654},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6188,\n",
       "     'learning_rate': 0.00012885525070955534},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6096,\n",
       "     'learning_rate': 0.00012866603595080417},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5805,\n",
       "     'learning_rate': 0.00012847682119205298},\n",
       "    {'epoch': 1, 'train_loss': 0.7, 'learning_rate': 0.0001282876064333018},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7473,\n",
       "     'learning_rate': 0.00012809839167455061},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6646,\n",
       "     'learning_rate': 0.00012790917691579945},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6259,\n",
       "     'learning_rate': 0.00012771996215704825},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6639,\n",
       "     'learning_rate': 0.00012753074739829708},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6818,\n",
       "     'learning_rate': 0.00012734153263954589},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6569,\n",
       "     'learning_rate': 0.00012715231788079472},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6164,\n",
       "     'learning_rate': 0.00012696310312204355},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6048,\n",
       "     'learning_rate': 0.00012677388836329233},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5771,\n",
       "     'learning_rate': 0.00012658467360454116},\n",
       "    {'epoch': 1, 'train_loss': 0.5327, 'learning_rate': 0.00012639545884579},\n",
       "    {'epoch': 1, 'train_loss': 0.5924, 'learning_rate': 0.0001262062440870388},\n",
       "    {'epoch': 1, 'train_loss': 0.567, 'learning_rate': 0.0001260170293282876},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5564,\n",
       "     'learning_rate': 0.00012582781456953643},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5477,\n",
       "     'learning_rate': 0.00012563859981078526},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6868,\n",
       "     'learning_rate': 0.00012544938505203407},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5652,\n",
       "     'learning_rate': 0.00012526017029328287},\n",
       "    {'epoch': 1, 'train_loss': 0.6492, 'learning_rate': 0.0001250709555345317},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6807,\n",
       "     'learning_rate': 0.00012488174077578053},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6197,\n",
       "     'learning_rate': 0.00012469252601702934},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6778,\n",
       "     'learning_rate': 0.00012450331125827814},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6255,\n",
       "     'learning_rate': 0.00012431409649952697},\n",
       "    {'epoch': 1, 'train_loss': 0.665, 'learning_rate': 0.00012412488174077578},\n",
       "    {'epoch': 1, 'train_loss': 0.5732, 'learning_rate': 0.0001239356669820246},\n",
       "    {'epoch': 1, 'train_loss': 0.6785, 'learning_rate': 0.0001237464522232734},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5253,\n",
       "     'learning_rate': 0.00012355723746452224},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6158,\n",
       "     'learning_rate': 0.00012336802270577105},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7074,\n",
       "     'learning_rate': 0.00012317880794701988},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6677,\n",
       "     'learning_rate': 0.00012298959318826868},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6101,\n",
       "     'learning_rate': 0.00012280037842951752},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4974,\n",
       "     'learning_rate': 0.00012261116367076632},\n",
       "    {'epoch': 1, 'train_loss': 0.62, 'learning_rate': 0.00012242194891201515},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7465,\n",
       "     'learning_rate': 0.00012223273415326396},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6045,\n",
       "     'learning_rate': 0.00012204351939451277},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7198,\n",
       "     'learning_rate': 0.00012185430463576159},\n",
       "    {'epoch': 1, 'train_loss': 0.576, 'learning_rate': 0.00012166508987701042},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4862,\n",
       "     'learning_rate': 0.00012147587511825921},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7478,\n",
       "     'learning_rate': 0.00012128666035950805},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6535,\n",
       "     'learning_rate': 0.00012109744560075686},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5377,\n",
       "     'learning_rate': 0.00012090823084200568},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5401,\n",
       "     'learning_rate': 0.00012071901608325451},\n",
       "    {'epoch': 1, 'train_loss': 0.5736, 'learning_rate': 0.0001205298013245033},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6754,\n",
       "     'learning_rate': 0.00012034058656575214},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6946,\n",
       "     'learning_rate': 0.00012015137180700095},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7109,\n",
       "     'learning_rate': 0.00011996215704824979},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5385,\n",
       "     'learning_rate': 0.00011977294228949858},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7175,\n",
       "     'learning_rate': 0.00011958372753074741},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6092,\n",
       "     'learning_rate': 0.00011939451277199623},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6687,\n",
       "     'learning_rate': 0.00011920529801324504},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5823,\n",
       "     'learning_rate': 0.00011901608325449385},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5984,\n",
       "     'learning_rate': 0.00011882686849574267},\n",
       "    {'epoch': 1, 'train_loss': 0.5733, 'learning_rate': 0.0001186376537369915},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5458,\n",
       "     'learning_rate': 0.00011844843897824032},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5162,\n",
       "     'learning_rate': 0.00011825922421948912},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6439,\n",
       "     'learning_rate': 0.00011807000946073794},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5194,\n",
       "     'learning_rate': 0.00011788079470198677},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5272,\n",
       "     'learning_rate': 0.00011769157994323559},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7036,\n",
       "     'learning_rate': 0.00011750236518448439},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5921,\n",
       "     'learning_rate': 0.00011731315042573321},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6199,\n",
       "     'learning_rate': 0.00011712393566698203},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6618,\n",
       "     'learning_rate': 0.00011693472090823086},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6209,\n",
       "     'learning_rate': 0.00011674550614947965},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7026,\n",
       "     'learning_rate': 0.00011655629139072848},\n",
       "    {'epoch': 1, 'train_loss': 0.6741, 'learning_rate': 0.0001163670766319773},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6438,\n",
       "     'learning_rate': 0.00011617786187322612},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5823,\n",
       "     'learning_rate': 0.00011598864711447492},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6565,\n",
       "     'learning_rate': 0.00011579943235572375},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5698,\n",
       "     'learning_rate': 0.00011561021759697257},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6391,\n",
       "     'learning_rate': 0.00011542100283822139},\n",
       "    {'epoch': 1, 'train_loss': 0.5046, 'learning_rate': 0.0001152317880794702},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6177,\n",
       "     'learning_rate': 0.00011504257332071901},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6582,\n",
       "     'learning_rate': 0.00011485335856196784},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5299,\n",
       "     'learning_rate': 0.00011466414380321666},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6574,\n",
       "     'learning_rate': 0.00011447492904446548},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6003,\n",
       "     'learning_rate': 0.00011428571428571428},\n",
       "    {'epoch': 1, 'train_loss': 0.6307, 'learning_rate': 0.0001140964995269631},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5144,\n",
       "     'learning_rate': 0.00011390728476821193},\n",
       "    {'epoch': 1, 'train_loss': 0.726, 'learning_rate': 0.00011371807000946075},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5615,\n",
       "     'learning_rate': 0.00011352885525070956},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5407,\n",
       "     'learning_rate': 0.00011333964049195837},\n",
       "    {'epoch': 1, 'train_loss': 0.7068, 'learning_rate': 0.0001131504257332072},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6677,\n",
       "     'learning_rate': 0.00011296121097445602},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6072,\n",
       "     'learning_rate': 0.00011277199621570483},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6875,\n",
       "     'learning_rate': 0.00011258278145695364},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6375,\n",
       "     'learning_rate': 0.00011239356669820246},\n",
       "    {'epoch': 1, 'train_loss': 0.6767, 'learning_rate': 0.0001122043519394513},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6521,\n",
       "     'learning_rate': 0.00011201513718070009},\n",
       "    {'epoch': 1, 'train_loss': 0.679, 'learning_rate': 0.00011182592242194892},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5518,\n",
       "     'learning_rate': 0.00011163670766319773},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6646,\n",
       "     'learning_rate': 0.00011144749290444657},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6871,\n",
       "     'learning_rate': 0.00011125827814569536},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6044,\n",
       "     'learning_rate': 0.00011106906338694419},\n",
       "    {'epoch': 1, 'train_loss': 0.678, 'learning_rate': 0.000110879848628193},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4912,\n",
       "     'learning_rate': 0.00011069063386944182},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6463,\n",
       "     'learning_rate': 0.00011050141911069063},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4899,\n",
       "     'learning_rate': 0.00011031220435193945},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6376,\n",
       "     'learning_rate': 0.00011012298959318828},\n",
       "    {'epoch': 1, 'train_loss': 0.6668, 'learning_rate': 0.0001099337748344371},\n",
       "    {'epoch': 1, 'train_loss': 0.5383, 'learning_rate': 0.0001097445600756859},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5805,\n",
       "     'learning_rate': 0.00010955534531693472},\n",
       "    {'epoch': 1, 'train_loss': 0.637, 'learning_rate': 0.00010936613055818355},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6675,\n",
       "     'learning_rate': 0.00010917691579943237},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6472,\n",
       "     'learning_rate': 0.00010898770104068119},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4716,\n",
       "     'learning_rate': 0.00010879848628192999},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5596,\n",
       "     'learning_rate': 0.00010860927152317881},\n",
       "    {'epoch': 1, 'train_loss': 0.581, 'learning_rate': 0.00010842005676442764},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6353,\n",
       "     'learning_rate': 0.00010823084200567646},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5136,\n",
       "     'learning_rate': 0.00010804162724692526},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4833,\n",
       "     'learning_rate': 0.00010785241248817408},\n",
       "    {'epoch': 1, 'train_loss': 0.6297, 'learning_rate': 0.0001076631977294229},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5999,\n",
       "     'learning_rate': 0.00010747398297067173},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6235,\n",
       "     'learning_rate': 0.00010728476821192053},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7003,\n",
       "     'learning_rate': 0.00010709555345316935},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5773,\n",
       "     'learning_rate': 0.00010690633869441817},\n",
       "    {'epoch': 1, 'train_loss': 0.6199, 'learning_rate': 0.000106717123935667},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6133,\n",
       "     'learning_rate': 0.00010652790917691579},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5339,\n",
       "     'learning_rate': 0.00010633869441816462},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7566,\n",
       "     'learning_rate': 0.00010614947965941344},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7127,\n",
       "     'learning_rate': 0.00010596026490066226},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5336,\n",
       "     'learning_rate': 0.00010577105014191106},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5375,\n",
       "     'learning_rate': 0.00010558183538315988},\n",
       "    {'epoch': 1, 'train_loss': 0.553, 'learning_rate': 0.00010539262062440871},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6914,\n",
       "     'learning_rate': 0.00010520340586565753},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5393,\n",
       "     'learning_rate': 0.00010501419110690634},\n",
       "    {'epoch': 1, 'train_loss': 0.661, 'learning_rate': 0.00010482497634815515},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6103,\n",
       "     'learning_rate': 0.00010463576158940399},\n",
       "    {'epoch': 1, 'train_loss': 0.5323, 'learning_rate': 0.0001044465468306528},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5141,\n",
       "     'learning_rate': 0.00010425733207190161},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7439,\n",
       "     'learning_rate': 0.00010406811731315043},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6464,\n",
       "     'learning_rate': 0.00010387890255439924},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6877,\n",
       "     'learning_rate': 0.00010368968779564808},\n",
       "    {'epoch': 1, 'train_loss': 0.634, 'learning_rate': 0.00010350047303689687},\n",
       "    {'epoch': 1, 'train_loss': 0.6929, 'learning_rate': 0.0001033112582781457},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6934,\n",
       "     'learning_rate': 0.00010312204351939452},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5078,\n",
       "     'learning_rate': 0.00010293282876064335},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6272,\n",
       "     'learning_rate': 0.00010274361400189216},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.7573,\n",
       "     'learning_rate': 0.00010255439924314097},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6584,\n",
       "     'learning_rate': 0.00010236518448438979},\n",
       "    {'epoch': 1, 'train_loss': 0.6553, 'learning_rate': 0.0001021759697256386},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5716,\n",
       "     'learning_rate': 0.00010198675496688744},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6657,\n",
       "     'learning_rate': 0.00010179754020813623},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5918,\n",
       "     'learning_rate': 0.00010160832544938506},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4868,\n",
       "     'learning_rate': 0.00010141911069063388},\n",
       "    {'epoch': 1, 'train_loss': 0.6084, 'learning_rate': 0.0001012298959318827},\n",
       "    {'epoch': 1, 'train_loss': 0.5911, 'learning_rate': 0.0001010406811731315},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.4834,\n",
       "     'learning_rate': 0.00010085146641438033},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.5382,\n",
       "     'learning_rate': 0.00010066225165562915},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6774,\n",
       "     'learning_rate': 0.00010047303689687797},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6632,\n",
       "     'learning_rate': 0.00010028382213812677},\n",
       "    {'epoch': 1,\n",
       "     'train_loss': 0.6171,\n",
       "     'learning_rate': 0.00010009460737937559},\n",
       "    {'epoch': 1, 'train_loss': 0.5467, 'learning_rate': 9.990539262062442e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5302, 'learning_rate': 9.971617786187322e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.616, 'learning_rate': 9.952696310312206e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.641, 'learning_rate': 9.933774834437086e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6593, 'learning_rate': 9.914853358561968e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5644, 'learning_rate': 9.89593188268685e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.476, 'learning_rate': 9.877010406811731e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5922, 'learning_rate': 9.858088930936613e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5511, 'learning_rate': 9.839167455061495e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6621, 'learning_rate': 9.820245979186377e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5753, 'learning_rate': 9.801324503311259e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5936, 'learning_rate': 9.78240302743614e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7087, 'learning_rate': 9.763481551561022e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7136, 'learning_rate': 9.744560075685904e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.451, 'learning_rate': 9.725638599810786e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6461, 'learning_rate': 9.706717123935666e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5803, 'learning_rate': 9.68779564806055e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6251, 'learning_rate': 9.668874172185431e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6662, 'learning_rate': 9.649952696310313e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5763, 'learning_rate': 9.631031220435195e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6582, 'learning_rate': 9.612109744560077e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6367, 'learning_rate': 9.593188268684958e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6482, 'learning_rate': 9.57426679280984e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5169, 'learning_rate': 9.555345316934722e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5665, 'learning_rate': 9.536423841059602e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5565, 'learning_rate': 9.517502365184486e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5224, 'learning_rate': 9.498580889309366e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5015, 'learning_rate': 9.479659413434249e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6005, 'learning_rate': 9.46073793755913e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5239, 'learning_rate': 9.441816461684013e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5565, 'learning_rate': 9.422894985808893e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5079, 'learning_rate': 9.403973509933775e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6711, 'learning_rate': 9.385052034058657e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.547, 'learning_rate': 9.366130558183539e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.459, 'learning_rate': 9.34720908230842e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6306, 'learning_rate': 9.328287606433302e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6694, 'learning_rate': 9.309366130558184e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6724, 'learning_rate': 9.290444654683066e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6865, 'learning_rate': 9.271523178807948e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7226, 'learning_rate': 9.25260170293283e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5521, 'learning_rate': 9.233680227057711e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4471, 'learning_rate': 9.214758751182593e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6585, 'learning_rate': 9.195837275307473e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5787, 'learning_rate': 9.176915799432357e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6173, 'learning_rate': 9.157994323557237e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6476, 'learning_rate': 9.13907284768212e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6819, 'learning_rate': 9.120151371807e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5459, 'learning_rate': 9.101229895931884e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6697, 'learning_rate': 9.082308420056766e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6919, 'learning_rate': 9.063386944181646e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.627, 'learning_rate': 9.044465468306529e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6956, 'learning_rate': 9.02554399243141e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6629, 'learning_rate': 9.006622516556293e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4904, 'learning_rate': 8.987701040681173e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6872, 'learning_rate': 8.968779564806056e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6205, 'learning_rate': 8.949858088930937e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5583, 'learning_rate': 8.93093661305582e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6321, 'learning_rate': 8.9120151371807e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.603, 'learning_rate': 8.893093661305582e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5659, 'learning_rate': 8.874172185430464e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6252, 'learning_rate': 8.855250709555346e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6526, 'learning_rate': 8.836329233680227e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5454, 'learning_rate': 8.817407757805109e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5168, 'learning_rate': 8.798486281929991e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6519, 'learning_rate': 8.779564806054873e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6308, 'learning_rate': 8.760643330179755e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5424, 'learning_rate': 8.741721854304636e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6286, 'learning_rate': 8.722800378429518e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6121, 'learning_rate': 8.7038789025544e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5513, 'learning_rate': 8.68495742667928e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6139, 'learning_rate': 8.666035950804164e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5884, 'learning_rate': 8.647114474929044e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5984, 'learning_rate': 8.628192999053927e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5714, 'learning_rate': 8.609271523178808e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6453, 'learning_rate': 8.590350047303691e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6414, 'learning_rate': 8.571428571428571e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6554, 'learning_rate': 8.552507095553453e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5756, 'learning_rate': 8.533585619678335e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6068, 'learning_rate': 8.514664143803217e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6315, 'learning_rate': 8.495742667928098e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6727, 'learning_rate': 8.47682119205298e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6254, 'learning_rate': 8.457899716177863e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6329, 'learning_rate': 8.438978240302744e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6158, 'learning_rate': 8.420056764427626e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5055, 'learning_rate': 8.401135288552507e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5843, 'learning_rate': 8.382213812677389e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6217, 'learning_rate': 8.363292336802271e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.605, 'learning_rate': 8.344370860927153e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5614, 'learning_rate': 8.325449385052035e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4813, 'learning_rate': 8.306527909176916e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6958, 'learning_rate': 8.287606433301798e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6317, 'learning_rate': 8.26868495742668e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.485, 'learning_rate': 8.249763481551562e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.568, 'learning_rate': 8.230842005676444e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5581, 'learning_rate': 8.211920529801324e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7168, 'learning_rate': 8.192999053926207e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5955, 'learning_rate': 8.174077578051088e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5187, 'learning_rate': 8.155156102175971e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5112, 'learning_rate': 8.136234626300851e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5231, 'learning_rate': 8.117313150425734e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5881, 'learning_rate': 8.098391674550615e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.587, 'learning_rate': 8.079470198675498e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5316, 'learning_rate': 8.060548722800378e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5945, 'learning_rate': 8.04162724692526e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5437, 'learning_rate': 8.022705771050142e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5266, 'learning_rate': 8.003784295175024e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4988, 'learning_rate': 7.984862819299906e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6613, 'learning_rate': 7.965941343424787e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6328, 'learning_rate': 7.947019867549669e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6089, 'learning_rate': 7.928098391674551e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6193, 'learning_rate': 7.909176915799433e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6204, 'learning_rate': 7.890255439924315e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6108, 'learning_rate': 7.871333964049196e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6406, 'learning_rate': 7.852412488174078e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6443, 'learning_rate': 7.83349101229896e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6493, 'learning_rate': 7.814569536423842e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6188, 'learning_rate': 7.795648060548723e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5296, 'learning_rate': 7.776726584673605e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.693, 'learning_rate': 7.757805108798487e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5454, 'learning_rate': 7.738883632923369e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7324, 'learning_rate': 7.719962157048251e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6943, 'learning_rate': 7.701040681173131e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.704, 'learning_rate': 7.682119205298014e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5716, 'learning_rate': 7.663197729422895e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5131, 'learning_rate': 7.644276253547778e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5382, 'learning_rate': 7.625354777672658e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7251, 'learning_rate': 7.606433301797541e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5439, 'learning_rate': 7.587511825922422e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6798, 'learning_rate': 7.568590350047304e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5692, 'learning_rate': 7.549668874172185e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4408, 'learning_rate': 7.530747398297067e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6929, 'learning_rate': 7.511825922421949e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.4934, 'learning_rate': 7.492904446546831e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6233, 'learning_rate': 7.473982970671713e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7146, 'learning_rate': 7.455061494796594e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6049, 'learning_rate': 7.436140018921476e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5596, 'learning_rate': 7.417218543046358e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.474, 'learning_rate': 7.39829706717124e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6474, 'learning_rate': 7.379375591296122e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6497, 'learning_rate': 7.360454115421002e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6809, 'learning_rate': 7.341532639545885e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5948, 'learning_rate': 7.322611163670766e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5167, 'learning_rate': 7.303689687795649e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6256, 'learning_rate': 7.284768211920529e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5741, 'learning_rate': 7.265846736045412e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.7087, 'learning_rate': 7.246925260170294e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6238, 'learning_rate': 7.228003784295176e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.674, 'learning_rate': 7.209082308420058e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.522, 'learning_rate': 7.190160832544938e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6829, 'learning_rate': 7.171239356669821e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5738, 'learning_rate': 7.152317880794702e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.672, 'learning_rate': 7.133396404919585e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6973, 'learning_rate': 7.114474929044465e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6128, 'learning_rate': 7.095553453169349e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6101, 'learning_rate': 7.076631977294229e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6562, 'learning_rate': 7.057710501419111e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5702, 'learning_rate': 7.038789025543993e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5834, 'learning_rate': 7.019867549668874e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.577, 'learning_rate': 7.000946073793756e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5032, 'learning_rate': 6.982024597918638e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5722, 'learning_rate': 6.96310312204352e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5361, 'learning_rate': 6.944181646168402e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6023, 'learning_rate': 6.925260170293283e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6269, 'learning_rate': 6.906338694418165e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6781, 'learning_rate': 6.887417218543047e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6029, 'learning_rate': 6.868495742667929e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5201, 'learning_rate': 6.849574266792809e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.5517, 'learning_rate': 6.830652790917692e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6226, 'learning_rate': 6.811731315042573e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.477, 'learning_rate': 6.792809839167456e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.558, 'learning_rate': 6.773888363292336e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6089, 'learning_rate': 6.75496688741722e-05},\n",
       "    {'epoch': 1, 'train_loss': 0.6956, 'learning_rate': 6.7360454115421e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5284, 'learning_rate': 6.717123935666982e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6284, 'learning_rate': 6.698202459791864e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5545, 'learning_rate': 6.679280983916745e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6061, 'learning_rate': 6.660359508041628e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.694, 'learning_rate': 6.641438032166509e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5309, 'learning_rate': 6.622516556291392e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6583, 'learning_rate': 6.603595080416273e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.731, 'learning_rate': 6.584673604541156e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6774, 'learning_rate': 6.565752128666036e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5004, 'learning_rate': 6.546830652790918e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5497, 'learning_rate': 6.5279091769158e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5951, 'learning_rate': 6.508987701040681e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.627, 'learning_rate': 6.490066225165563e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6295, 'learning_rate': 6.471144749290445e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5789, 'learning_rate': 6.452223273415327e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5551, 'learning_rate': 6.433301797540209e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6536, 'learning_rate': 6.41438032166509e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.569, 'learning_rate': 6.395458845789972e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.653, 'learning_rate': 6.376537369914854e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6061, 'learning_rate': 6.357615894039736e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6568, 'learning_rate': 6.338694418164616e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.587, 'learning_rate': 6.3197729422895e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6649, 'learning_rate': 6.30085146641438e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6069, 'learning_rate': 6.281929990539263e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5905, 'learning_rate': 6.263008514664143e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5464, 'learning_rate': 6.244087038789027e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5401, 'learning_rate': 6.225165562913907e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6434, 'learning_rate': 6.206244087038789e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5402, 'learning_rate': 6.18732261116367e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7259, 'learning_rate': 6.168401135288552e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6312, 'learning_rate': 6.149479659413434e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5725, 'learning_rate': 6.130558183538316e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6477, 'learning_rate': 6.111636707663198e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6503,\n",
       "     'learning_rate': 6.0927152317880796e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5005, 'learning_rate': 6.073793755912961e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6499, 'learning_rate': 6.054872280037843e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5638, 'learning_rate': 6.035950804162726e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5904, 'learning_rate': 6.017029328287607e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5041, 'learning_rate': 5.998107852412489e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5289,\n",
       "     'learning_rate': 5.9791863765373704e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5186, 'learning_rate': 5.960264900662252e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4823, 'learning_rate': 5.941343424787133e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5429, 'learning_rate': 5.922421948912016e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.616, 'learning_rate': 5.903500473036897e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5578,\n",
       "     'learning_rate': 5.8845789971617794e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4707,\n",
       "     'learning_rate': 5.8656575212866605e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6951, 'learning_rate': 5.846736045411543e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4989, 'learning_rate': 5.827814569536424e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6266, 'learning_rate': 5.808893093661306e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6073, 'learning_rate': 5.789971617786188e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.605, 'learning_rate': 5.7710501419110695e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.7182,\n",
       "     'learning_rate': 5.7521286660359506e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5528, 'learning_rate': 5.733207190160833e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.537, 'learning_rate': 5.714285714285714e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5257,\n",
       "     'learning_rate': 5.6953642384105966e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5998, 'learning_rate': 5.676442762535478e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6089, 'learning_rate': 5.65752128666036e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5353,\n",
       "     'learning_rate': 5.6385998107852414e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6733, 'learning_rate': 5.619678334910123e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5257, 'learning_rate': 5.600756859035004e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5318, 'learning_rate': 5.581835383159887e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6153, 'learning_rate': 5.562913907284768e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4982, 'learning_rate': 5.54399243140965e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6137,\n",
       "     'learning_rate': 5.5250709555345314e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4865, 'learning_rate': 5.506149479659414e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7153, 'learning_rate': 5.487228003784295e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.519, 'learning_rate': 5.4683065279091775e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5903, 'learning_rate': 5.449385052034059e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5518,\n",
       "     'learning_rate': 5.4304635761589404e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5901, 'learning_rate': 5.411542100283823e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5903, 'learning_rate': 5.392620624408704e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6588,\n",
       "     'learning_rate': 5.3736991485335865e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5916,\n",
       "     'learning_rate': 5.3547776726584676e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4716, 'learning_rate': 5.33585619678335e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.594, 'learning_rate': 5.316934720908231e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5407, 'learning_rate': 5.298013245033113e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6047, 'learning_rate': 5.279091769157994e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5707,\n",
       "     'learning_rate': 5.2601702932828766e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4325, 'learning_rate': 5.241248817407758e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5792, 'learning_rate': 5.22232734153264e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5257, 'learning_rate': 5.203405865657521e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5447, 'learning_rate': 5.184484389782404e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6549, 'learning_rate': 5.165562913907285e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.7151,\n",
       "     'learning_rate': 5.1466414380321673e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5931,\n",
       "     'learning_rate': 5.1277199621570485e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.583, 'learning_rate': 5.10879848628193e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5096,\n",
       "     'learning_rate': 5.0898770104068114e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6207, 'learning_rate': 5.070955534531694e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7049, 'learning_rate': 5.052034058656575e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5186,\n",
       "     'learning_rate': 5.0331125827814574e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6223,\n",
       "     'learning_rate': 5.0141911069063386e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5409, 'learning_rate': 4.995269631031221e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5827, 'learning_rate': 4.976348155156103e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5921, 'learning_rate': 4.957426679280984e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.571, 'learning_rate': 4.938505203405866e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4262,\n",
       "     'learning_rate': 4.9195837275307475e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6487, 'learning_rate': 4.900662251655629e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5477, 'learning_rate': 4.881740775780511e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5219, 'learning_rate': 4.862819299905393e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6061, 'learning_rate': 4.843897824030275e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5412,\n",
       "     'learning_rate': 4.8249763481551565e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.638, 'learning_rate': 4.806054872280038e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5534, 'learning_rate': 4.78713339640492e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6278, 'learning_rate': 4.768211920529801e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6198, 'learning_rate': 4.749290444654683e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5586, 'learning_rate': 4.730368968779565e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6179,\n",
       "     'learning_rate': 4.7114474929044466e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5198,\n",
       "     'learning_rate': 4.6925260170293284e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6211, 'learning_rate': 4.67360454115421e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4829, 'learning_rate': 4.654683065279092e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5018, 'learning_rate': 4.635761589403974e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5278,\n",
       "     'learning_rate': 4.6168401135288556e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5805, 'learning_rate': 4.597918637653737e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6159,\n",
       "     'learning_rate': 4.5789971617786185e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6746, 'learning_rate': 4.5600756859035e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5141, 'learning_rate': 4.541154210028383e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.554, 'learning_rate': 4.5222327341532645e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6088,\n",
       "     'learning_rate': 4.5033112582781463e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5897, 'learning_rate': 4.484389782403028e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5829, 'learning_rate': 4.46546830652791e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7194, 'learning_rate': 4.446546830652791e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5937, 'learning_rate': 4.427625354777673e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6048,\n",
       "     'learning_rate': 4.4087038789025546e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5082,\n",
       "     'learning_rate': 4.3897824030274364e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5045, 'learning_rate': 4.370860927152318e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4452, 'learning_rate': 4.3519394512772e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5974, 'learning_rate': 4.333017975402082e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6816,\n",
       "     'learning_rate': 4.3140964995269636e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6902,\n",
       "     'learning_rate': 4.2951750236518454e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.474, 'learning_rate': 4.2762535477767265e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5432, 'learning_rate': 4.257332071901608e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5432, 'learning_rate': 4.23841059602649e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5836, 'learning_rate': 4.219489120151372e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5205, 'learning_rate': 4.200567644276254e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6391,\n",
       "     'learning_rate': 4.1816461684011355e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4874, 'learning_rate': 4.162724692526017e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6092, 'learning_rate': 4.143803216650899e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6468, 'learning_rate': 4.124881740775781e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5367, 'learning_rate': 4.105960264900662e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5843, 'learning_rate': 4.087038789025544e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6912,\n",
       "     'learning_rate': 4.0681173131504256e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6006,\n",
       "     'learning_rate': 4.0491958372753074e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5594, 'learning_rate': 4.030274361400189e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5546, 'learning_rate': 4.011352885525071e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6571, 'learning_rate': 3.992431409649953e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.7071,\n",
       "     'learning_rate': 3.9735099337748346e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6158,\n",
       "     'learning_rate': 3.9545884578997164e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6888, 'learning_rate': 3.935666982024598e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7282, 'learning_rate': 3.91674550614948e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5246, 'learning_rate': 3.897824030274362e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.588, 'learning_rate': 3.8789025543992435e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.564, 'learning_rate': 3.8599810785241253e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5645, 'learning_rate': 3.841059602649007e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4568, 'learning_rate': 3.822138126773889e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7379, 'learning_rate': 3.803216650898771e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5836, 'learning_rate': 3.784295175023652e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4898,\n",
       "     'learning_rate': 3.7653736991485336e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5097,\n",
       "     'learning_rate': 3.7464522232734154e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4849, 'learning_rate': 3.727530747398297e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4951, 'learning_rate': 3.708609271523179e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.571, 'learning_rate': 3.689687795648061e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6749,\n",
       "     'learning_rate': 3.6707663197729426e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5873,\n",
       "     'learning_rate': 3.6518448438978244e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5597, 'learning_rate': 3.632923368022706e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6096, 'learning_rate': 3.614001892147588e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5905, 'learning_rate': 3.595080416272469e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6452, 'learning_rate': 3.576158940397351e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.52, 'learning_rate': 3.557237464522233e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5669,\n",
       "     'learning_rate': 3.5383159886471145e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6143, 'learning_rate': 3.519394512771996e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.615, 'learning_rate': 3.500473036896878e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7021, 'learning_rate': 3.48155156102176e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5319, 'learning_rate': 3.462630085146642e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.667, 'learning_rate': 3.4437086092715235e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4153,\n",
       "     'learning_rate': 3.4247871333964046e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5863,\n",
       "     'learning_rate': 3.4058656575212864e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4831, 'learning_rate': 3.386944181646168e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5691, 'learning_rate': 3.36802270577105e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6538, 'learning_rate': 3.349101229895932e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6781, 'learning_rate': 3.330179754020814e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5538, 'learning_rate': 3.311258278145696e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5471, 'learning_rate': 3.292336802270578e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5042, 'learning_rate': 3.273415326395459e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.583, 'learning_rate': 3.254493850520341e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.605, 'learning_rate': 3.2355723746452225e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4775, 'learning_rate': 3.216650898770104e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5482, 'learning_rate': 3.197729422894986e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6206, 'learning_rate': 3.178807947019868e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.655, 'learning_rate': 3.15988647114475e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4896,\n",
       "     'learning_rate': 3.1409649952696315e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6921, 'learning_rate': 3.122043519394513e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.588, 'learning_rate': 3.1031220435193944e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6051, 'learning_rate': 3.084200567644276e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5933, 'learning_rate': 3.065279091769158e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6078,\n",
       "     'learning_rate': 3.0463576158940398e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6722,\n",
       "     'learning_rate': 3.0274361400189216e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5069,\n",
       "     'learning_rate': 3.0085146641438034e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.493, 'learning_rate': 2.9895931882686852e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5102,\n",
       "     'learning_rate': 2.9706717123935667e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5889,\n",
       "     'learning_rate': 2.9517502365184484e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5747,\n",
       "     'learning_rate': 2.9328287606433302e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.551, 'learning_rate': 2.913907284768212e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6082, 'learning_rate': 2.894985808893094e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5198,\n",
       "     'learning_rate': 2.8760643330179753e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5473, 'learning_rate': 2.857142857142857e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5812, 'learning_rate': 2.838221381267739e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5735,\n",
       "     'learning_rate': 2.8192999053926207e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5349, 'learning_rate': 2.800378429517502e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5843, 'learning_rate': 2.781456953642384e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.481, 'learning_rate': 2.7625354777672657e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5132,\n",
       "     'learning_rate': 2.7436140018921475e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5219,\n",
       "     'learning_rate': 2.7246925260170297e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6218,\n",
       "     'learning_rate': 2.7057710501419114e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.533, 'learning_rate': 2.6868495742667932e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4554, 'learning_rate': 2.667928098391675e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6537,\n",
       "     'learning_rate': 2.6490066225165565e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5541,\n",
       "     'learning_rate': 2.6300851466414383e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5673, 'learning_rate': 2.61116367076632e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6161, 'learning_rate': 2.592242194891202e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.573, 'learning_rate': 2.5733207190160837e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.557, 'learning_rate': 2.554399243140965e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5513, 'learning_rate': 2.535477767265847e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6374,\n",
       "     'learning_rate': 2.5165562913907287e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.631, 'learning_rate': 2.4976348155156105e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5935, 'learning_rate': 2.478713339640492e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5245,\n",
       "     'learning_rate': 2.4597918637653738e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6982,\n",
       "     'learning_rate': 2.4408703878902556e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6806,\n",
       "     'learning_rate': 2.4219489120151374e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7139, 'learning_rate': 2.403027436140019e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6985,\n",
       "     'learning_rate': 2.3841059602649006e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4861,\n",
       "     'learning_rate': 2.3651844843897824e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6088,\n",
       "     'learning_rate': 2.3462630085146642e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5368, 'learning_rate': 2.327341532639546e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5165,\n",
       "     'learning_rate': 2.3084200567644278e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6014,\n",
       "     'learning_rate': 2.2894985808893092e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4907,\n",
       "     'learning_rate': 2.2705771050141914e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6024,\n",
       "     'learning_rate': 2.2516556291390732e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6208, 'learning_rate': 2.232734153263955e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5548,\n",
       "     'learning_rate': 2.2138126773888364e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4791,\n",
       "     'learning_rate': 2.1948912015137182e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5512, 'learning_rate': 2.1759697256386e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5511,\n",
       "     'learning_rate': 2.1570482497634818e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.592, 'learning_rate': 2.1381267738883633e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5505, 'learning_rate': 2.119205298013245e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5384, 'learning_rate': 2.100283822138127e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6854,\n",
       "     'learning_rate': 2.0813623462630086e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5238,\n",
       "     'learning_rate': 2.0624408703878904e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7029, 'learning_rate': 2.043519394512772e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5481,\n",
       "     'learning_rate': 2.0245979186376537e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4838,\n",
       "     'learning_rate': 2.0056764427625355e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6929,\n",
       "     'learning_rate': 1.9867549668874173e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5817, 'learning_rate': 1.967833491012299e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5471, 'learning_rate': 1.948912015137181e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5466,\n",
       "     'learning_rate': 1.9299905392620627e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6642,\n",
       "     'learning_rate': 1.9110690633869445e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5426, 'learning_rate': 1.892147587511826e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4658,\n",
       "     'learning_rate': 1.8732261116367077e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6025,\n",
       "     'learning_rate': 1.8543046357615895e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5972,\n",
       "     'learning_rate': 1.8353831598864713e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6178, 'learning_rate': 1.816461684011353e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5725,\n",
       "     'learning_rate': 1.7975402081362346e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.7055,\n",
       "     'learning_rate': 1.7786187322611164e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4902, 'learning_rate': 1.759697256385998e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5774, 'learning_rate': 1.74077578051088e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4963,\n",
       "     'learning_rate': 1.7218543046357617e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5562,\n",
       "     'learning_rate': 1.7029328287606432e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6995, 'learning_rate': 1.684011352885525e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5762, 'learning_rate': 1.665089877010407e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5345, 'learning_rate': 1.646168401135289e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6287,\n",
       "     'learning_rate': 1.6272469252601704e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.4876, 'learning_rate': 1.608325449385052e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5499, 'learning_rate': 1.589403973509934e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6024,\n",
       "     'learning_rate': 1.5704824976348158e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4982,\n",
       "     'learning_rate': 1.5515610217596972e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.5706, 'learning_rate': 1.532639545884579e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5495,\n",
       "     'learning_rate': 1.5137180700094608e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6264,\n",
       "     'learning_rate': 1.4947965941343426e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.638, 'learning_rate': 1.4758751182592242e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.7393, 'learning_rate': 1.456953642384106e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6433,\n",
       "     'learning_rate': 1.4380321665089876e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6299,\n",
       "     'learning_rate': 1.4191106906338694e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.6324, 'learning_rate': 1.400189214758751e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.4808,\n",
       "     'learning_rate': 1.3812677388836329e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5603,\n",
       "     'learning_rate': 1.3623462630085148e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6212,\n",
       "     'learning_rate': 1.3434247871333966e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.5146,\n",
       "     'learning_rate': 1.3245033112582782e-05},\n",
       "    {'epoch': 2, 'train_loss': 0.509, 'learning_rate': 1.30558183538316e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.7028,\n",
       "     'learning_rate': 1.2866603595080418e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6674,\n",
       "     'learning_rate': 1.2677388836329235e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6989,\n",
       "     'learning_rate': 1.2488174077578053e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6533,\n",
       "     'learning_rate': 1.2298959318826869e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6259,\n",
       "     'learning_rate': 1.2109744560075687e-05},\n",
       "    {'epoch': 2,\n",
       "     'train_loss': 0.6339,\n",
       "     'learning_rate': 1.1920529801324503e-05},\n",
       "    ...],\n",
       "   'final_metrics': {'final_loss': 0.4559,\n",
       "    'accuracy': 0,\n",
       "    'training_time_minutes': 44.06}}},\n",
       " '_metadata': {'note': 'Real training metrics from standard fine-tuning',\n",
       "  'exported_at': '2025-12-06T19:36:19.511712'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_standard_metrics(trainer, trainer_stats, output_path='training_metrics.json'):\n",
    "    \"\"\"\n",
    "    Export training metrics from standard fine-tuning notebook\n",
    "\n",
    "    Args:\n",
    "        trainer: SFTTrainer object after training\n",
    "        trainer_stats: Result from trainer.train()\n",
    "        output_path: Output file path\n",
    "    \"\"\"\n",
    "    # Load existing metrics file if it exists\n",
    "    try:\n",
    "        with open(output_path, 'r') as f:\n",
    "            metrics_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        metrics_data = {\"approaches\": {}}\n",
    "\n",
    "    # Extract epoch data from log history\n",
    "    epochs_data = []\n",
    "    current_epoch = 0\n",
    "\n",
    "    if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):\n",
    "        for entry in trainer.state.log_history:\n",
    "            # Look for entries with 'loss' and 'epoch'\n",
    "            if 'loss' in entry:\n",
    "                epoch_num = entry.get('epoch', current_epoch)\n",
    "                if epoch_num > current_epoch:\n",
    "                    current_epoch = epoch_num\n",
    "\n",
    "                epoch_data = {\n",
    "                    \"epoch\": int(epoch_num) if epoch_num else len(epochs_data) + 1,\n",
    "                    \"train_loss\": float(entry.get('loss', entry.get('train_loss', 0))),\n",
    "                    \"learning_rate\": float(entry.get('learning_rate', 0))\n",
    "                }\n",
    "                epochs_data.append(epoch_data)\n",
    "\n",
    "    # If no epoch data found, create from final stats\n",
    "    if not epochs_data and hasattr(trainer_stats, 'metrics'):\n",
    "        final_loss = trainer_stats.metrics.get('train_loss', 0)\n",
    "        epochs_data = [{\n",
    "            \"epoch\": 1,\n",
    "            \"train_loss\": float(final_loss),\n",
    "            \"learning_rate\": 0\n",
    "        }]\n",
    "\n",
    "    # Get training time from trainer stats\n",
    "    training_time_seconds = trainer_stats.metrics.get('train_runtime', 0) if hasattr(trainer_stats, 'metrics') else 0\n",
    "    training_time_minutes = training_time_seconds / 60 if training_time_seconds > 0 else 0\n",
    "\n",
    "    # Get final loss\n",
    "    final_loss = epochs_data[-1]['train_loss'] if epochs_data else 0\n",
    "\n",
    "    # Update metrics data\n",
    "    metrics_data['approaches']['standard'] = {\n",
    "        \"name\": \"Standard Fine-tuning\",\n",
    "        \"epochs\": epochs_data,\n",
    "        \"final_metrics\": {\n",
    "            \"final_loss\": final_loss,\n",
    "            \"accuracy\": 0,  # Fill manually if you have evaluation results\n",
    "            \"training_time_minutes\": round(training_time_minutes, 2)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metrics_data['_metadata'] = {\n",
    "        \"note\": \"Real training metrics from standard fine-tuning\",\n",
    "        \"exported_at\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "    print(f\"âœ“ Exported {len(epochs_data)} epochs\")\n",
    "    print(f\"âœ“ Training time: {training_time_minutes:.2f} minutes\")\n",
    "    print(f\"âœ“ Final loss: {final_loss:.4f}\")\n",
    "    print(f\"âœ“ Saved to: {output_path}\")\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "export_standard_metrics(trainer, trainer_stats, 'standard_training_metrics.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
