{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimized Fine-Tuning for GWU Course Search\n",
        "\n",
        "This notebook implements an **optimized fine-tuning approach** with:\n",
        "- Train/Validation Split\n",
        "- Early Stopping\n",
        "- Better Hyperparameters (Cosine Annealing, Optimal LR)\n",
        "- Evaluation Metrics\n",
        "- Better Data Handling\n",
        "- Improved Inference Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model with Optimized Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Optimized settings\n",
        "max_seq_length = 2048  # Sufficient for course Q&A\n",
        "dtype = None  # Auto-detect (Bfloat16 for Ampere+)\n",
        "load_in_4bit = True  # Memory efficient\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure LoRA with Optimized Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # Increased from 16 for better capacity\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Match r value (r:alpha = 1:1 is optimal)\n",
        "    lora_dropout = 0.05,  # Small dropout for regularization\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and Prepare Dataset with Train/Validation Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"course_finetune.jsonl\", split=\"train\")\n",
        "\n",
        "# Setup Llama 3.1 template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Format dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# Create train/validation split (80/20)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=3407)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample training example:\")\n",
        "print(train_dataset[0][\"text\"][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Optimized Training Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Optimized training configuration\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,  # Validation set for early stopping\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,  # False for better quality with variable-length sequences\n",
        "    args = SFTConfig(\n",
        "        # Batch size - optimized for A100\n",
        "        per_device_train_batch_size = 4,  # Increased from 2\n",
        "        per_device_eval_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,  # Adjusted to maintain effective batch size\n",
        "        \n",
        "        # Learning rate - optimal for LoRA fine-tuning\n",
        "        learning_rate = 1e-4,  # Lower, more stable (was 2e-4)\n",
        "        lr_scheduler_type = \"cosine\",  # Cosine annealing (better than linear)\n",
        "        warmup_ratio = 0.1,  # 10% warmup (better than fixed steps)\n",
        "        \n",
        "        # Training duration\n",
        "        num_train_epochs = 5,  # More epochs, but early stopping will prevent overfitting\n",
        "        max_steps = -1,  # Use epochs instead\n",
        "        \n",
        "        # Optimization\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,  # Slightly higher for regularization\n",
        "        adam_beta1 = 0.9,\n",
        "        adam_beta2 = 0.999,\n",
        "        \n",
        "        # Evaluation and logging\n",
        "        eval_strategy = \"steps\",  # Evaluate during training\n",
        "        eval_steps = 100,  # Evaluate every 100 steps\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 200,\n",
        "        logging_steps = 10,  # More frequent logging\n",
        "        report_to = \"none\",\n",
        "        \n",
        "        # Output\n",
        "        output_dir = \"outputs_optimized\",\n",
        "        seed = 3407,\n",
        "        fp16 = False,  # Use bfloat16 (handled by unsloth)\n",
        "        bf16 = True,\n",
        "        \n",
        "        # Early stopping\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"eval_loss\",  # Lower is better\n",
        "        greater_is_better = False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Add early stopping callback\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience = 3,  # Stop if no improvement for 3 evaluations\n",
        "    early_stopping_threshold = 0.001,  # Minimum improvement threshold\n",
        ")\n",
        "trainer.add_callback(early_stopping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model with Early Stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds ({trainer_stats.metrics['train_runtime']/60:.2f} minutes)\")\n",
        "print(f\"Peak reserved memory: {used_memory} GB ({used_percentage}%)\")\n",
        "print(f\"Training memory: {used_memory_for_lora} GB ({lora_percentage}%)\")\n",
        "print(f\"\\nFinal training loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
        "print(f\"Best validation loss: {trainer_stats.metrics.get('eval_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Inference with Optimized Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"Who Teaches Machine Learning?\",\n",
        "    \"What courses are available on Tuesdays?\",\n",
        "    \"Tell me about CSCI 1012.\",\n",
        "    \"What is the schedule for CSCI 4244?\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant providing information about GWU Computer Science courses for Spring 2026.\"},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    attention_mask = torch.ones_like(inputs)\n",
        "    \n",
        "    from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids = inputs,\n",
        "        attention_mask = attention_mask,\n",
        "        streamer = text_streamer,\n",
        "        max_new_tokens = 256,\n",
        "        temperature = 0.1,  # Low temperature for factual accuracy\n",
        "        do_sample = True,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "        eos_token_id = tokenizer.eos_token_id,\n",
        "        repetition_penalty = 1.2,  # Prevent repetition\n",
        "        top_p = 0.9,  # Nucleus sampling\n",
        "    )\n",
        "    \n",
        "    # Clean output\n",
        "    output_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    print(f\"\\nClean Answer: {output_text.strip()}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"lora_model_optimized\")\n",
        "tokenizer.save_pretrained(\"lora_model_optimized\")\n",
        "print(\"LoRA adapters saved to 'lora_model_optimized'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Merged Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to merged 16-bit model for easier deployment\n",
        "save_method = \"merged_16bit\"\n",
        "print(f\"Saving {save_method} locally...\")\n",
        "model.save_pretrained_merged(\"merged_model_optimized\", tokenizer, save_method=save_method)\n",
        "print(\"Merged model saved to 'merged_model_optimized'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload to Hugging Face Hub (Optional)\n",
        "\n",
        "If you want to share your model or use it in other environments, you can upload it to Hugging Face Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Upload to Hugging Face Hub\n",
        "push_to_hub = False  # Set to True to enable upload\n",
        "hf_repo_name = \"itsmepraks/gwcoursesfinetuned-optimized\"  # Change to your username/repo\n",
        "hf_token = None  # Will try to get from environment or Colab secrets\n",
        "\n",
        "if push_to_hub:\n",
        "    from huggingface_hub import HfApi\n",
        "    import os\n",
        "    \n",
        "    # Try to get token from Colab secrets or environment\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        hf_token = userdata.get(\"HF_TOKEN\")\n",
        "        print(\"Loaded HF_TOKEN from Colab Secrets.\")\n",
        "    except:\n",
        "        hf_token = os.getenv(\"HF_TOKEN\")\n",
        "        if not hf_token:\n",
        "            raise ValueError(\"HF_TOKEN not found! Please add 'HF_TOKEN' to Colab Secrets or set as environment variable.\")\n",
        "    \n",
        "    api = HfApi(token=hf_token)\n",
        "    \n",
        "    # Create repository if it doesn't exist\n",
        "    print(f\"Ensuring repository {hf_repo_name} exists...\")\n",
        "    api.create_repo(repo_id=hf_repo_name, repo_type=\"model\", exist_ok=True)\n",
        "    \n",
        "    # Upload merged model\n",
        "    print(f\"Uploading merged model to {hf_repo_name}...\")\n",
        "    api.upload_folder(\n",
        "        folder_path=\"merged_model_optimized\",\n",
        "        repo_id=hf_repo_name,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… Model uploaded successfully to https://huggingface.co/{hf_repo_name}\")\n",
        "else:\n",
        "    print(\"Skipping Hugging Face upload. Set push_to_hub=True to enable.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
