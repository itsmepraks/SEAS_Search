{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Graph Question Answering System for GW Courses\n",
        "\n",
        "This notebook implements a **comprehensive Knowledge Graph QA system** with:\n",
        "- Knowledge Graph Construction (Courses, Professors, Topics, Prerequisites)\n",
        "- Multi-hop Reasoning Training Data\n",
        "- Graph Attention Network (GAT) for Graph Retrieval\n",
        "- Hybrid Retrieval + Generation Architecture (RAG)\n",
        "- Comprehensive Evaluation Metrics\n",
        "- Prerequisites and Topic Extraction\n",
        "- Structured Output Format\n",
        "- Data Augmentation\n",
        "- Performance Monitoring\n",
        "\n",
        "**Project Goal**: Build an intelligent question-answering system over a custom knowledge graph of GW courses with multi-hop reasoning capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install evaluate\n",
        "!pip install torch-geometric networkx pandas numpy scikit-learn\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import networkx as nx\n",
        "from collections import defaultdict, Counter\n",
        "from typing import Dict, List, Tuple, Set, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from datasets import load_dataset, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"Warning: spaCy model not loaded. Topic extraction may be limited.\")\n",
        "    nlp = None\n",
        "\n",
        "print(\"✅ Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load course data\n",
        "courses_df = pd.read_csv(\"data/spring_2026_courses.csv\")\n",
        "bulletin_df = pd.read_csv(\"data/bulletin_courses.csv\")\n",
        "\n",
        "print(f\"Loaded {len(courses_df)} course sections\")\n",
        "print(f\"Loaded {len(bulletin_df)} course descriptions\")\n",
        "\n",
        "# Create course code to description mapping\n",
        "course_descriptions = {}\n",
        "for _, row in bulletin_df.iterrows():\n",
        "    code = str(row['course_code']).strip()\n",
        "    desc = str(row.get('description', '')).strip()\n",
        "    if desc and desc != 'nan':\n",
        "        course_descriptions[code] = desc\n",
        "\n",
        "print(f\"✅ Data loaded: {len(course_descriptions)} course descriptions available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prerequisites and Topic Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_prerequisites(description: str) -> List[str]:\n",
        "    \"\"\"Extract prerequisite course codes from description.\"\"\"\n",
        "    if not description or description == 'nan':\n",
        "        return []\n",
        "    \n",
        "    prerequisites = []\n",
        "    # Pattern: \"Prerequisites: CSCI 1112\" or \"Prerequisite: CSCI 1112\"\n",
        "    patterns = [\n",
        "        r'[Pp]rerequisite[s]?[:\\s]+([A-Z]{2,}\\s+\\d{4})',\n",
        "        r'[Pp]rerequisite[s]?[:\\s]+([A-Z]{2,}\\s+\\d{4}[A-Z]?)',\n",
        "        r'([A-Z]{2,}\\s+\\d{4}[A-Z]?)\\s+with\\s+a\\s+minimum\\s+grade',\n",
        "        r'([A-Z]{2,}\\s+\\d{4}[A-Z]?)\\s+or\\s+([A-Z]{2,}\\s+\\d{4}[A-Z]?)',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, description)\n",
        "        if matches:\n",
        "            if isinstance(matches[0], tuple):\n",
        "                prerequisites.extend([m for m in matches if m])\n",
        "            else:\n",
        "                prerequisites.extend(matches)\n",
        "    \n",
        "    # Clean and normalize\n",
        "    prerequisites = [p.strip() for p in prerequisites if p.strip()]\n",
        "    return list(set(prerequisites))\n",
        "\n",
        "def extract_topics(description: str, course_code: str) -> List[str]:\n",
        "    \"\"\"Extract topics from course description using NLP.\"\"\"\n",
        "    if not description or description == 'nan':\n",
        "        return []\n",
        "    \n",
        "    topics = []\n",
        "    \n",
        "    # Common CS topics/keywords\n",
        "    cs_topics = [\n",
        "        'machine learning', 'deep learning', 'neural networks', 'computer vision',\n",
        "        'natural language processing', 'nlp', 'data structures', 'algorithms',\n",
        "        'database', 'software engineering', 'operating systems', 'networks',\n",
        "        'distributed systems', 'security', 'cryptography', 'web development',\n",
        "        'mobile development', 'artificial intelligence', 'ai', 'robotics',\n",
        "        'graphics', 'game development', 'cloud computing', 'parallel computing',\n",
        "        'compilers', 'programming languages', 'theory', 'optimization'\n",
        "    ]\n",
        "    \n",
        "    description_lower = description.lower()\n",
        "    for topic in cs_topics:\n",
        "        if topic in description_lower:\n",
        "            topics.append(topic)\n",
        "    \n",
        "    # Use spaCy for named entity recognition if available\n",
        "    if nlp:\n",
        "        doc = nlp(description)\n",
        "        # Extract technical terms (nouns and noun phrases)\n",
        "        for chunk in doc.noun_chunks:\n",
        "            text = chunk.text.lower()\n",
        "            if len(text) > 3 and text not in stopwords.words('english'):\n",
        "                # Filter for technical terms\n",
        "                if any(keyword in text for keyword in ['algorithm', 'system', 'structure', 'model', 'framework']):\n",
        "                    topics.append(text)\n",
        "    \n",
        "    return list(set(topics))\n",
        "\n",
        "# Extract prerequisites and topics for all courses\n",
        "course_prerequisites = {}\n",
        "course_topics = {}\n",
        "\n",
        "for course_code, description in course_descriptions.items():\n",
        "    course_prerequisites[course_code] = extract_prerequisites(description)\n",
        "    course_topics[course_code] = extract_topics(description, course_code)\n",
        "\n",
        "# Print statistics\n",
        "total_with_prereqs = sum(1 for v in course_prerequisites.values() if v)\n",
        "total_with_topics = sum(1 for v in course_topics.values() if v)\n",
        "\n",
        "print(f\"✅ Extracted prerequisites for {total_with_prereqs} courses\")\n",
        "print(f\"✅ Extracted topics for {total_with_topics} courses\")\n",
        "print(f\"\\nSample prerequisites: {dict(list(course_prerequisites.items())[:5])}\")\n",
        "print(f\"\\nSample topics: {dict(list(course_topics.items())[:5])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeGraph:\n",
        "    \"\"\"Knowledge Graph for GW Courses with nodes and edges.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()  # Directed graph\n",
        "        self.course_nodes = {}  # course_code -> node_id\n",
        "        self.professor_nodes = {}  # professor_name -> node_id\n",
        "        self.topic_nodes = {}  # topic -> node_id\n",
        "        self.node_features = {}  # node_id -> features\n",
        "        self.edge_types = {}  # (source, target) -> edge_type\n",
        "        self.node_id_counter = 0\n",
        "        \n",
        "    def add_node(self, node_type: str, node_id: str, features: Dict = None):\n",
        "        \"\"\"Add a node to the graph.\"\"\"\n",
        "        if node_id not in self.graph:\n",
        "            self.graph.add_node(node_id, node_type=node_type, **{**(features or {})})\n",
        "            self.node_features[node_id] = features or {}\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def add_edge(self, source: str, target: str, edge_type: str, weight: float = 1.0):\n",
        "        \"\"\"Add an edge to the graph.\"\"\"\n",
        "        if source in self.graph and target in self.graph:\n",
        "            self.graph.add_edge(source, target, edge_type=edge_type, weight=weight)\n",
        "            self.edge_types[(source, target)] = edge_type\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def build_from_data(self, courses_df: pd.DataFrame, course_descriptions: Dict,\n",
        "                       course_prerequisites: Dict, course_topics: Dict):\n",
        "        \"\"\"Build knowledge graph from course data.\"\"\"\n",
        "        print(\"Building knowledge graph...\")\n",
        "        \n",
        "        # Add course nodes\n",
        "        unique_courses = courses_df['subject_code'].unique()\n",
        "        for course_code in unique_courses:\n",
        "            course_code = str(course_code).strip()\n",
        "            if course_code and course_code != 'nan':\n",
        "                node_id = f\"course_{course_code}\"\n",
        "                description = course_descriptions.get(course_code, \"\")\n",
        "                features = {\n",
        "                    'code': course_code,\n",
        "                    'description': description,\n",
        "                    'has_prerequisites': len(course_prerequisites.get(course_code, [])) > 0,\n",
        "                    'topics': course_topics.get(course_code, [])\n",
        "                }\n",
        "                self.add_node('course', node_id, features)\n",
        "                self.course_nodes[course_code] = node_id\n",
        "        \n",
        "        # Add professor nodes\n",
        "        unique_professors = courses_df['instructor'].dropna().unique()\n",
        "        for prof in unique_professors:\n",
        "            prof = str(prof).strip()\n",
        "            if prof and prof != 'nan':\n",
        "                node_id = f\"prof_{prof.replace(' ', '_').replace(',', '')}\"\n",
        "                if self.add_node('professor', node_id, {'name': prof}):\n",
        "                    self.professor_nodes[prof] = node_id\n",
        "        \n",
        "        # Add topic nodes\n",
        "        all_topics = set()\n",
        "        for topics in course_topics.values():\n",
        "            all_topics.update(topics)\n",
        "        \n",
        "        for topic in all_topics:\n",
        "            node_id = f\"topic_{topic.replace(' ', '_')}\"\n",
        "            if self.add_node('topic', node_id, {'name': topic}):\n",
        "                self.topic_nodes[topic] = node_id\n",
        "        \n",
        "        # Add edges: taught_by\n",
        "        for _, row in courses_df.iterrows():\n",
        "            course_code = str(row['subject_code']).strip()\n",
        "            prof = str(row.get('instructor', '')).strip()\n",
        "            \n",
        "            if course_code in self.course_nodes and prof in self.professor_nodes:\n",
        "                course_node = self.course_nodes[course_code]\n",
        "                prof_node = self.professor_nodes[prof]\n",
        "                self.add_edge(course_node, prof_node, 'taught_by')\n",
        "        \n",
        "        # Add edges: prerequisite\n",
        "        for course_code, prereqs in course_prerequisites.items():\n",
        "            if course_code in self.course_nodes:\n",
        "                course_node = self.course_nodes[course_code]\n",
        "                for prereq_code in prereqs:\n",
        "                    prereq_node_id = f\"course_{prereq_code}\"\n",
        "                    if prereq_node_id in self.graph:\n",
        "                        self.add_edge(course_node, prereq_node_id, 'prerequisite')\n",
        "        \n",
        "        # Add edges: covers_topic\n",
        "        for course_code, topics in course_topics.items():\n",
        "            if course_code in self.course_nodes:\n",
        "                course_node = self.course_nodes[course_code]\n",
        "                for topic in topics:\n",
        "                    topic_node_id = f\"topic_{topic.replace(' ', '_')}\"\n",
        "                    if topic_node_id in self.graph:\n",
        "                        self.add_edge(course_node, topic_node_id, 'covers_topic')\n",
        "        \n",
        "        print(f\"✅ Graph built: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\")\n",
        "        print(f\"   - Courses: {len(self.course_nodes)}\")\n",
        "        print(f\"   - Professors: {len(self.professor_nodes)}\")\n",
        "        print(f\"   - Topics: {len(self.topic_nodes)}\")\n",
        "    \n",
        "    def get_subgraph(self, start_nodes: List[str], max_hops: int = 2) -> nx.DiGraph:\n",
        "        \"\"\"Get subgraph starting from given nodes with max_hops depth.\"\"\"\n",
        "        subgraph_nodes = set(start_nodes)\n",
        "        \n",
        "        for _ in range(max_hops):\n",
        "            new_nodes = set()\n",
        "            for node in subgraph_nodes:\n",
        "                # Get neighbors (both incoming and outgoing)\n",
        "                new_nodes.update(self.graph.successors(node))\n",
        "                new_nodes.update(self.graph.predecessors(node))\n",
        "            subgraph_nodes.update(new_nodes)\n",
        "        \n",
        "        return self.graph.subgraph(subgraph_nodes)\n",
        "    \n",
        "    def find_paths(self, source: str, target: str, max_length: int = 3) -> List[List[str]]:\n",
        "        \"\"\"Find all paths from source to target.\"\"\"\n",
        "        try:\n",
        "            paths = list(nx.all_simple_paths(self.graph, source, target, cutoff=max_length))\n",
        "            return paths\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "# Build knowledge graph\n",
        "kg = KnowledgeGraph()\n",
        "kg.build_from_data(courses_df, course_descriptions, course_prerequisites, course_topics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphAttentionNetwork(nn.Module):\n",
        "    \"\"\"Graph Attention Network for scoring relevant graph paths.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_heads: int = 4, num_layers: int = 2):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        \n",
        "        # First layer\n",
        "        self.gat_layers.append(GATConv(input_dim, hidden_dim, heads=num_heads, dropout=0.1, concat=True))\n",
        "        \n",
        "        # Intermediate layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.gat_layers.append(GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=0.1, concat=True))\n",
        "        \n",
        "        # Final layer (no concatenation)\n",
        "        if num_layers > 1:\n",
        "            self.gat_layers.append(GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=0.1, concat=False))\n",
        "        \n",
        "        self.output_proj = nn.Linear(hidden_dim, 1)  # Score output\n",
        "        \n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        \"\"\"Forward pass through GAT layers.\"\"\"\n",
        "        for i, gat_layer in enumerate(self.gat_layers):\n",
        "            x = gat_layer(x, edge_index)\n",
        "            if i < len(self.gat_layers) - 1:\n",
        "                x = torch.relu(x)\n",
        "        \n",
        "        # Output score\n",
        "        scores = self.output_proj(x).squeeze(-1)\n",
        "        return scores, x\n",
        "\n",
        "class GraphRetriever:\n",
        "    \"\"\"Retriever that uses GAT to find relevant graph subgraphs for queries.\"\"\"\n",
        "    \n",
        "    def __init__(self, knowledge_graph: KnowledgeGraph, gat_model: Optional[GraphAttentionNetwork] = None):\n",
        "        self.kg = knowledge_graph\n",
        "        self.gat_model = gat_model\n",
        "        \n",
        "    def retrieve_subgraph(self, query: str, query_entities: List[str], max_hops: int = 2) -> nx.DiGraph:\n",
        "        \"\"\"Retrieve relevant subgraph for a query.\"\"\"\n",
        "        # Find starting nodes from query entities\n",
        "        start_nodes = []\n",
        "        for entity in query_entities:\n",
        "            # Try to match course codes\n",
        "            if entity in self.kg.course_nodes:\n",
        "                start_nodes.append(self.kg.course_nodes[entity])\n",
        "            # Try to match professors\n",
        "            for prof_name, node_id in self.kg.professor_nodes.items():\n",
        "                if entity.lower() in prof_name.lower() or prof_name.lower() in entity.lower():\n",
        "                    start_nodes.append(node_id)\n",
        "            # Try to match topics\n",
        "            for topic, node_id in self.kg.topic_nodes.items():\n",
        "                if entity.lower() in topic.lower():\n",
        "                    start_nodes.append(node_id)\n",
        "        \n",
        "        if not start_nodes:\n",
        "            # If no entities found, return empty subgraph\n",
        "            return nx.DiGraph()\n",
        "        \n",
        "        # Get subgraph\n",
        "        subgraph = self.kg.get_subgraph(start_nodes, max_hops=max_hops)\n",
        "        return subgraph\n",
        "    \n",
        "    def format_subgraph_context(self, subgraph: nx.DiGraph) -> str:\n",
        "        \"\"\"Format subgraph as text context for LLM.\"\"\"\n",
        "        if subgraph.number_of_nodes() == 0:\n",
        "            return \"No relevant graph information found.\"\n",
        "        \n",
        "        context_parts = []\n",
        "        \n",
        "        # Group by edge type\n",
        "        edges_by_type = defaultdict(list)\n",
        "        for u, v, data in subgraph.edges(data=True):\n",
        "            edge_type = data.get('edge_type', 'unknown')\n",
        "            edges_by_type[edge_type].append((u, v))\n",
        "        \n",
        "        # Format prerequisite relationships\n",
        "        if 'prerequisite' in edges_by_type:\n",
        "            prereqs = []\n",
        "            for u, v in edges_by_type['prerequisite']:\n",
        "                course_u = subgraph.nodes[u].get('code', u)\n",
        "                course_v = subgraph.nodes[v].get('code', v)\n",
        "                prereqs.append(f\"{course_v} is a prerequisite for {course_u}\")\n",
        "            if prereqs:\n",
        "                context_parts.append(\"Prerequisites: \" + \"; \".join(prereqs[:10]))\n",
        "        \n",
        "        # Format taught_by relationships\n",
        "        if 'taught_by' in edges_by_type:\n",
        "            taught_by = []\n",
        "            for u, v in edges_by_type['taught_by']:\n",
        "                course = subgraph.nodes[u].get('code', u)\n",
        "                prof = subgraph.nodes[v].get('name', v)\n",
        "                taught_by.append(f\"{course} is taught by {prof}\")\n",
        "            if taught_by:\n",
        "                context_parts.append(\"Instructors: \" + \"; \".join(taught_by[:10]))\n",
        "        \n",
        "        # Format covers_topic relationships\n",
        "        if 'covers_topic' in edges_by_type:\n",
        "            topics = []\n",
        "            for u, v in edges_by_type['covers_topic']:\n",
        "                course = subgraph.nodes[u].get('code', u)\n",
        "                topic = subgraph.nodes[v].get('name', v)\n",
        "                topics.append(f\"{course} covers {topic}\")\n",
        "            if topics:\n",
        "                context_parts.append(\"Topics: \" + \"; \".join(topics[:10]))\n",
        "        \n",
        "        return \"\\n\".join(context_parts) if context_parts else \"Graph context available.\"\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = GraphRetriever(kg)\n",
        "print(\"✅ Graph Retriever initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_query(query: str) -> List[str]:\n",
        "    \"\"\"Extract potential entities (course codes, professor names) from query.\"\"\"\n",
        "    entities = []\n",
        "    \n",
        "    # Extract course codes (e.g., \"CSCI 6212\")\n",
        "    course_pattern = r'([A-Z]{2,}\\s+\\d{4}[A-Z]?)'\n",
        "    course_matches = re.findall(course_pattern, query)\n",
        "    entities.extend(course_matches)\n",
        "    \n",
        "    # Extract topic mentions\n",
        "    for topic in kg.topic_nodes.keys():\n",
        "        if topic.lower() in query.lower():\n",
        "            entities.append(topic)\n",
        "    \n",
        "    return list(set(entities))\n",
        "\n",
        "def generate_multi_hop_questions(kg: KnowledgeGraph, num_examples: int = 100) -> List[Dict]:\n",
        "    \"\"\"Generate multi-hop reasoning questions from the knowledge graph.\"\"\"\n",
        "    questions = []\n",
        "    \n",
        "    # Type 1: Prerequisite chain questions\n",
        "    # \"Which courses should I take to prepare for X if I've completed Y?\"\n",
        "    courses_with_prereqs = [code for code, prereqs in course_prerequisites.items() if prereqs]\n",
        "    \n",
        "    for _ in range(min(num_examples // 4, len(courses_with_prereqs))):\n",
        "        target_course = np.random.choice(courses_with_prereqs)\n",
        "        prereqs = course_prerequisites[target_course]\n",
        "        if prereqs:\n",
        "            completed_course = np.random.choice(prereqs)\n",
        "            \n",
        "            # Find all prerequisites in the chain\n",
        "            all_prereqs = []\n",
        "            def get_all_prereqs(course):\n",
        "                if course in course_prerequisites:\n",
        "                    for p in course_prerequisites[course]:\n",
        "                        if p not in all_prereqs:\n",
        "                            all_prereqs.append(p)\n",
        "                            get_all_prereqs(p)\n",
        "            \n",
        "            get_all_prereqs(target_course)\n",
        "            \n",
        "            query = f\"Which courses should I take to prepare for {target_course} if I've completed {completed_course}?\"\n",
        "            \n",
        "            # Build answer\n",
        "            remaining_prereqs = [p for p in all_prereqs if p != completed_course]\n",
        "            if remaining_prereqs:\n",
        "                answer = f\"To prepare for {target_course}, you should also take: {', '.join(remaining_prereqs[:5])}.\"\n",
        "            else:\n",
        "                answer = f\"After completing {completed_course}, you are ready to take {target_course}.\"\n",
        "            \n",
        "            # Get graph context\n",
        "            entities = extract_entities_from_query(query)\n",
        "            subgraph = retriever.retrieve_subgraph(query, entities, max_hops=3)\n",
        "            graph_context = retriever.format_subgraph_context(subgraph)\n",
        "            \n",
        "            questions.append({\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'graph_context': graph_context,\n",
        "                'reasoning_path': f\"{completed_course} -> {target_course}\",\n",
        "                'type': 'prerequisite_chain'\n",
        "            })\n",
        "    \n",
        "    # Type 2: Professor intersection questions\n",
        "    # \"Which professors teach courses that are prerequisites for both X and Y?\"\n",
        "    courses_list = list(kg.course_nodes.keys())[:50]  # Limit for performance\n",
        "    \n",
        "    for _ in range(min(num_examples // 4, 50)):\n",
        "        if len(courses_list) < 2:\n",
        "            break\n",
        "        course1, course2 = np.random.choice(courses_list, 2, replace=False)\n",
        "        \n",
        "        # Find professors teaching prerequisites of both\n",
        "        prereqs1 = set(course_prerequisites.get(course1, []))\n",
        "        prereqs2 = set(course_prerequisites.get(course2, []))\n",
        "        common_prereqs = prereqs1.intersection(prereqs2)\n",
        "        \n",
        "        if common_prereqs:\n",
        "            query = f\"Which professors teach courses that are prerequisites for both {course1} and {course2}?\"\n",
        "            \n",
        "            # Find professors teaching common prerequisites\n",
        "            profs = []\n",
        "            for _, row in courses_df.iterrows():\n",
        "                if str(row['subject_code']).strip() in common_prereqs:\n",
        "                    prof = str(row.get('instructor', '')).strip()\n",
        "                    if prof and prof != 'nan':\n",
        "                        profs.append(prof)\n",
        "            \n",
        "            if profs:\n",
        "                answer = f\"Professors teaching prerequisites for both courses include: {', '.join(set(profs)[:5])}.\"\n",
        "            else:\n",
        "                answer = f\"No professors found teaching common prerequisites for {course1} and {course2}.\"\n",
        "            \n",
        "            entities = extract_entities_from_query(query)\n",
        "            subgraph = retriever.retrieve_subgraph(query, entities, max_hops=3)\n",
        "            graph_context = retriever.format_subgraph_context(subgraph)\n",
        "            \n",
        "            questions.append({\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'graph_context': graph_context,\n",
        "                'reasoning_path': f\"{course1} ∩ {course2} prerequisites\",\n",
        "                'type': 'professor_intersection'\n",
        "            })\n",
        "    \n",
        "    # Type 3: Topic-based questions\n",
        "    # \"What courses cover machine learning and are prerequisites for computer vision?\"\n",
        "    topics_list = list(kg.topic_nodes.keys())[:20]\n",
        "    \n",
        "    for _ in range(min(num_examples // 4, 30)):\n",
        "        if len(topics_list) < 2:\n",
        "            break\n",
        "        topic1, topic2 = np.random.choice(topics_list, 2, replace=False)\n",
        "        \n",
        "        # Find courses covering topic1\n",
        "        courses_topic1 = [code for code, topics in course_topics.items() if topic1 in topics]\n",
        "        # Find courses that are prerequisites for courses covering topic2\n",
        "        courses_topic2 = [code for code, topics in course_topics.items() if topic2 in topics]\n",
        "        \n",
        "        relevant_courses = []\n",
        "        for c1 in courses_topic1:\n",
        "            prereqs = course_prerequisites.get(c1, [])\n",
        "            if any(c2 in prereqs for c2 in courses_topic2):\n",
        "                relevant_courses.append(c1)\n",
        "        \n",
        "        if relevant_courses:\n",
        "            query = f\"What courses cover {topic1} and are prerequisites for courses covering {topic2}?\"\n",
        "            answer = f\"Courses covering {topic1} that are prerequisites for {topic2} courses include: {', '.join(relevant_courses[:5])}.\"\n",
        "            \n",
        "            entities = extract_entities_from_query(query)\n",
        "            subgraph = retriever.retrieve_subgraph(query, entities, max_hops=3)\n",
        "            graph_context = retriever.format_subgraph_context(subgraph)\n",
        "            \n",
        "            questions.append({\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'graph_context': graph_context,\n",
        "                'reasoning_path': f\"{topic1} -> {topic2}\",\n",
        "                'type': 'topic_based'\n",
        "            })\n",
        "    \n",
        "    # Type 4: Multi-hop path questions\n",
        "    # \"What is the path from course X to course Y through prerequisites?\"\n",
        "    for _ in range(min(num_examples // 4, 30)):\n",
        "        if len(courses_list) < 2:\n",
        "            break\n",
        "        source, target = np.random.choice(courses_list, 2, replace=False)\n",
        "        \n",
        "        source_node = kg.course_nodes.get(source)\n",
        "        target_node = kg.course_nodes.get(target)\n",
        "        \n",
        "        if source_node and target_node:\n",
        "            paths = kg.find_paths(source_node, target_node, max_length=4)\n",
        "            if paths:\n",
        "                path = paths[0]  # Take first path\n",
        "                path_courses = [kg.graph.nodes[n].get('code', n) for n in path if kg.graph.nodes[n].get('node_type') == 'course']\n",
        "                \n",
        "                query = f\"What is the prerequisite path from {source} to {target}?\"\n",
        "                answer = f\"The path from {source} to {target} is: {' -> '.join(path_courses)}.\"\n",
        "                \n",
        "                entities = extract_entities_from_query(query)\n",
        "                subgraph = retriever.retrieve_subgraph(query, entities, max_hops=4)\n",
        "                graph_context = retriever.format_subgraph_context(subgraph)\n",
        "                \n",
        "                questions.append({\n",
        "                    'query': query,\n",
        "                    'answer': answer,\n",
        "                    'graph_context': graph_context,\n",
        "                    'reasoning_path': ' -> '.join(path_courses),\n",
        "                    'type': 'multi_hop_path'\n",
        "                })\n",
        "    \n",
        "    return questions\n",
        "\n",
        "# Generate multi-hop questions\n",
        "print(\"Generating multi-hop reasoning questions...\")\n",
        "multi_hop_questions = generate_multi_hop_questions(kg, num_examples=200)\n",
        "print(f\"✅ Generated {len(multi_hop_questions)} multi-hop questions\")\n",
        "print(f\"\\nSample questions:\")\n",
        "for i, q in enumerate(multi_hop_questions[:3]):\n",
        "    print(f\"\\n{i+1}. {q['query']}\")\n",
        "    print(f\"   Answer: {q['answer']}\")\n",
        "    print(f\"   Type: {q['type']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_training_example(query: str, answer: str, graph_context: str, \n",
        "                                reasoning_path: str = \"\", include_structure: bool = True) -> Dict:\n",
        "    \"\"\"Create a RAG training example with graph context.\"\"\"\n",
        "    \n",
        "    # Build system message with graph context\n",
        "    system_content = \"\"\"You are a helpful assistant providing information about GWU Computer Science courses for Spring 2026.\n",
        "You have access to a knowledge graph with course relationships, prerequisites, instructors, and topics.\n",
        "Use the provided graph context to answer questions accurately.\"\"\"\n",
        "    \n",
        "    # Build user message with graph context\n",
        "    if graph_context and graph_context != \"No relevant graph information found.\":\n",
        "        user_content = f\"\"\"Graph Context:\n",
        "{graph_context}\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "    else:\n",
        "        user_content = f\"Question: {query}\"\n",
        "    \n",
        "    # Build assistant response with structured format\n",
        "    if include_structure and reasoning_path:\n",
        "        assistant_content = f\"\"\"Reasoning Path: {reasoning_path}\n",
        "\n",
        "Answer: {answer}\"\"\"\n",
        "    else:\n",
        "        assistant_content = answer\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ],\n",
        "        \"graph_context\": graph_context,\n",
        "        \"reasoning_path\": reasoning_path,\n",
        "        \"query_type\": \"multi_hop\" if reasoning_path else \"simple\"\n",
        "    }\n",
        "\n",
        "# Load existing simple Q&A data\n",
        "existing_dataset = []\n",
        "try:\n",
        "    with open(\"data/course_finetune.jsonl\", 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                existing_dataset.append(json.loads(line))\n",
        "    print(f\"Loaded {len(existing_dataset)} existing examples\")\n",
        "except:\n",
        "    print(\"No existing dataset found, starting fresh\")\n",
        "\n",
        "# Convert existing examples to RAG format (without graph context for simple ones)\n",
        "rag_dataset = []\n",
        "for example in existing_dataset[:500]:  # Limit to avoid too much data\n",
        "    messages = example.get('messages', [])\n",
        "    if len(messages) >= 3:\n",
        "        user_msg = messages[1].get('content', '')\n",
        "        assistant_msg = messages[2].get('content', '')\n",
        "        \n",
        "        # Extract entities and get graph context\n",
        "        entities = extract_entities_from_query(user_msg)\n",
        "        if entities:\n",
        "            subgraph = retriever.retrieve_subgraph(user_msg, entities, max_hops=2)\n",
        "            graph_context = retriever.format_subgraph_context(subgraph)\n",
        "        else:\n",
        "            graph_context = \"\"\n",
        "        \n",
        "        rag_example = create_rag_training_example(\n",
        "            user_msg, assistant_msg, graph_context, \n",
        "            reasoning_path=\"\", include_structure=False\n",
        "        )\n",
        "        rag_dataset.append(rag_example)\n",
        "\n",
        "# Add multi-hop questions\n",
        "for q in multi_hop_questions:\n",
        "    rag_example = create_rag_training_example(\n",
        "        q['query'], q['answer'], q['graph_context'],\n",
        "        reasoning_path=q.get('reasoning_path', ''), include_structure=True\n",
        "    )\n",
        "    rag_dataset.append(rag_example)\n",
        "\n",
        "print(f\"✅ Created RAG dataset with {len(rag_dataset)} examples\")\n",
        "print(f\"   - Simple Q&A: {len([x for x in rag_dataset if x['query_type'] == 'simple'])}\")\n",
        "print(f\"   - Multi-hop: {len([x for x in rag_dataset if x['query_type'] == 'multi_hop'])}\")\n",
        "\n",
        "# Save dataset\n",
        "output_file = \"data/course_finetune_kg_rag.jsonl\"\n",
        "with open(output_file, 'w') as f:\n",
        "    for example in rag_dataset:\n",
        "        f.write(json.dumps(example) + \"\\n\")\n",
        "print(f\"✅ Saved dataset to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Setup chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"✅ LoRA configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prepare Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"data/course_finetune_kg_rag.jsonl\", split=\"train\")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Format dataset with chat template.\"\"\"\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) \n",
        "             for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Format dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Train/validation split (80/20)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=3407)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"✅ Dataset prepared:\")\n",
        "print(f\"   Train examples: {len(train_dataset)}\")\n",
        "print(f\"   Validation examples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample training example:\")\n",
        "print(train_dataset[0][\"text\"][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Configuration with Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "import evaluate\n",
        "\n",
        "# Load evaluation metrics\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Compute BLEU\n",
        "    bleu_results = bleu_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[ref] for ref in decoded_labels]\n",
        "    )\n",
        "    \n",
        "    # Compute ROUGE\n",
        "    rouge_results = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "# Training configuration\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        # Batch size\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        \n",
        "        # Learning rate\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        \n",
        "        # Training duration\n",
        "        num_train_epochs=5,\n",
        "        max_steps=-1,\n",
        "        \n",
        "        # Optimization\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.999,\n",
        "        \n",
        "        # Evaluation and logging\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "        \n",
        "        # Output\n",
        "        output_dir=\"outputs_kg_qa\",\n",
        "        seed=3407,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        \n",
        "        # Early stopping\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        \n",
        "        # Metrics\n",
        "        # Note: compute_metrics will be called during evaluation\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Add early stopping\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001,\n",
        ")\n",
        "trainer.add_callback(early_stopping)\n",
        "\n",
        "print(\"✅ Training configuration complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU memory\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# Train\n",
        "print(\"\\nStarting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Training statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "\n",
        "print(f\"\\n✅ Training completed!\")\n",
        "print(f\"Runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds ({trainer_stats.metrics['train_runtime']/60:.2f} minutes)\")\n",
        "print(f\"Peak reserved memory: {used_memory} GB ({used_percentage}%)\")\n",
        "print(f\"Training memory: {used_memory_for_lora} GB\")\n",
        "print(f\"Final training loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Evaluation Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match(prediction: str, reference: str) -> bool:\n",
        "    \"\"\"Check if prediction exactly matches reference.\"\"\"\n",
        "    return prediction.strip().lower() == reference.strip().lower()\n",
        "\n",
        "def f1_score(prediction: str, reference: str) -> float:\n",
        "    \"\"\"Compute F1 score between prediction and reference.\"\"\"\n",
        "    pred_tokens = set(prediction.lower().split())\n",
        "    ref_tokens = set(reference.lower().split())\n",
        "    \n",
        "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    intersection = pred_tokens.intersection(ref_tokens)\n",
        "    if len(intersection) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    precision = len(intersection) / len(pred_tokens)\n",
        "    recall = len(intersection) / len(ref_tokens)\n",
        "    \n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def evaluate_qa_predictions(predictions: List[str], references: List[str]) -> Dict:\n",
        "    \"\"\"Evaluate QA predictions with multiple metrics.\"\"\"\n",
        "    em_scores = [exact_match(p, r) for p, r in zip(predictions, references)]\n",
        "    f1_scores = [f1_score(p, r) for p, r in zip(predictions, references)]\n",
        "    \n",
        "    # Compute BLEU and ROUGE\n",
        "    bleu_results = bleu_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=[[ref] for ref in references]\n",
        "    )\n",
        "    rouge_results = rouge_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=references\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"exact_match\": np.mean(em_scores),\n",
        "        \"f1\": np.mean(f1_scores),\n",
        "        \"bleu\": bleu_results[\"bleu\"],\n",
        "        \"rouge1\": rouge_results[\"rouge1\"],\n",
        "        \"rouge2\": rouge_results[\"rouge2\"],\n",
        "        \"rougeL\": rouge_results[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "# Create test set with complex queries\n",
        "test_queries = [\n",
        "    {\n",
        "        \"query\": \"Which courses should I take to prepare for CSCI 6364 if I've completed CSCI 1112?\",\n",
        "        \"expected_entities\": [\"CSCI 6364\", \"CSCI 1112\"],\n",
        "        \"type\": \"prerequisite_chain\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Who teaches Machine Learning?\",\n",
        "        \"expected_entities\": [\"machine learning\"],\n",
        "        \"type\": \"simple\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What courses cover computer vision and are prerequisites for deep learning courses?\",\n",
        "        \"expected_entities\": [\"computer vision\", \"deep learning\"],\n",
        "        \"type\": \"topic_based\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Tell me about CSCI 1012.\",\n",
        "        \"expected_entities\": [\"CSCI 1012\"],\n",
        "        \"type\": \"simple\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Which professors teach courses that are prerequisites for both CSCI 6364 and CSCI 6444?\",\n",
        "        \"expected_entities\": [\"CSCI 6364\", \"CSCI 6444\"],\n",
        "        \"type\": \"professor_intersection\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✅ Evaluation framework ready with {len(test_queries)} test queries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Inference Pipeline with Graph Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def answer_with_graph_retrieval(query: str, max_new_tokens: int = 256) -> Dict:\n",
        "    \"\"\"Answer query using graph retrieval + LLM generation.\"\"\"\n",
        "    \n",
        "    # Step 1: Extract entities from query\n",
        "    entities = extract_entities_from_query(query)\n",
        "    \n",
        "    # Step 2: Retrieve relevant subgraph\n",
        "    subgraph = retriever.retrieve_subgraph(query, entities, max_hops=3)\n",
        "    graph_context = retriever.format_subgraph_context(subgraph)\n",
        "    \n",
        "    # Step 3: Build prompt with graph context\n",
        "    system_content = \"\"\"You are a helpful assistant providing information about GWU Computer Science courses for Spring 2026.\n",
        "You have access to a knowledge graph with course relationships, prerequisites, instructors, and topics.\n",
        "Use the provided graph context to answer questions accurately.\"\"\"\n",
        "    \n",
        "    if graph_context and graph_context != \"No relevant graph information found.\":\n",
        "        user_content = f\"\"\"Graph Context:\n",
        "{graph_context}\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "    else:\n",
        "        user_content = f\"Question: {query}\"\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "    \n",
        "    # Step 4: Generate answer\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    attention_mask = torch.ones_like(inputs)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    \n",
        "    # Decode answer\n",
        "    output_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": output_text.strip(),\n",
        "        \"graph_context\": graph_context,\n",
        "        \"entities_found\": entities,\n",
        "        \"subgraph_size\": subgraph.number_of_nodes() if subgraph else 0\n",
        "    }\n",
        "\n",
        "# Test inference\n",
        "print(\"Testing inference pipeline...\\n\")\n",
        "for i, test_query in enumerate(test_queries[:3], 1):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Test {i}: {test_query['query']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = answer_with_graph_retrieval(test_query['query'])\n",
        "    \n",
        "    print(f\"Entities found: {result['entities_found']}\")\n",
        "    print(f\"Subgraph nodes: {result['subgraph_size']}\")\n",
        "    print(f\"\\nGraph Context:\\n{result['graph_context'][:200]}...\")\n",
        "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Performance Monitoring and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test queries...\\n\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "graph_retrieval_stats = {\n",
        "    \"total_queries\": 0,\n",
        "    \"queries_with_graph_context\": 0,\n",
        "    \"avg_subgraph_size\": [],\n",
        "    \"entities_extracted\": 0\n",
        "}\n",
        "\n",
        "for test_query in test_queries:\n",
        "    result = answer_with_graph_retrieval(test_query['query'])\n",
        "    predictions.append(result['answer'])\n",
        "    \n",
        "    # For evaluation, we'd need ground truth answers\n",
        "    # For now, we'll use a placeholder\n",
        "    references.append(\"\")  # Would be actual ground truth\n",
        "    \n",
        "    # Track graph retrieval stats\n",
        "    graph_retrieval_stats[\"total_queries\"] += 1\n",
        "    if result['graph_context'] and result['graph_context'] != \"No relevant graph information found.\":\n",
        "        graph_retrieval_stats[\"queries_with_graph_context\"] += 1\n",
        "    graph_retrieval_stats[\"avg_subgraph_size\"].append(result['subgraph_size'])\n",
        "    graph_retrieval_stats[\"entities_extracted\"] += len(result['entities_found'])\n",
        "\n",
        "# Print statistics\n",
        "print(\"Graph Retrieval Statistics:\")\n",
        "print(f\"  Total queries: {graph_retrieval_stats['total_queries']}\")\n",
        "print(f\"  Queries with graph context: {graph_retrieval_stats['queries_with_graph_context']}\")\n",
        "print(f\"  Average subgraph size: {np.mean(graph_retrieval_stats['avg_subgraph_size']):.2f}\")\n",
        "print(f\"  Total entities extracted: {graph_retrieval_stats['entities_extracted']}\")\n",
        "print(f\"  Average entities per query: {graph_retrieval_stats['entities_extracted'] / graph_retrieval_stats['total_queries']:.2f}\")\n",
        "\n",
        "# Note: Full evaluation with ground truth would require labeled test set\n",
        "print(\"\\n✅ Performance monitoring complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Data Augmentation for Synthetic Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_question(query: str, answer: str) -> List[Dict]:\n",
        "    \"\"\"Generate variations of a question for data augmentation.\"\"\"\n",
        "    variations = []\n",
        "    \n",
        "    # Variation 1: Paraphrase\n",
        "    # Simple paraphrasing (in production, use a paraphrase model)\n",
        "    if \"which courses\" in query.lower():\n",
        "        variations.append({\n",
        "            \"query\": query.replace(\"Which courses\", \"What courses\"),\n",
        "            \"answer\": answer,\n",
        "            \"type\": \"paraphrase\"\n",
        "        })\n",
        "    \n",
        "    # Variation 2: Question type change\n",
        "    if \"who teaches\" in query.lower():\n",
        "        variations.append({\n",
        "            \"query\": query.replace(\"Who teaches\", \"Which professor teaches\"),\n",
        "            \"answer\": answer,\n",
        "            \"type\": \"question_type\"\n",
        "        })\n",
        "    \n",
        "    # Variation 3: Add context\n",
        "    if \"if I've completed\" in query.lower():\n",
        "        variations.append({\n",
        "            \"query\": query.replace(\"if I've completed\", \"assuming I have completed\"),\n",
        "            \"answer\": answer,\n",
        "            \"type\": \"context_variation\"\n",
        "        })\n",
        "    \n",
        "    return variations\n",
        "\n",
        "def generate_synthetic_questions_from_graph(kg: KnowledgeGraph, num_synthetic: int = 50) -> List[Dict]:\n",
        "    \"\"\"Generate synthetic questions by exploring the graph structure.\"\"\"\n",
        "    synthetic = []\n",
        "    \n",
        "    # Generate questions by following graph paths\n",
        "    courses_list = list(kg.course_nodes.keys())[:30]\n",
        "    \n",
        "    for _ in range(num_synthetic):\n",
        "        # Random walk on graph\n",
        "        start_course = np.random.choice(courses_list)\n",
        "        start_node = kg.course_nodes[start_course]\n",
        "        \n",
        "        # Get neighbors\n",
        "        neighbors = list(kg.graph.successors(start_node))[:3]\n",
        "        if neighbors:\n",
        "            target_node = np.random.choice(neighbors)\n",
        "            edge_data = kg.graph.get_edge_data(start_node, target_node)\n",
        "            \n",
        "            if edge_data:\n",
        "                edge_type = edge_data.get('edge_type', '')\n",
        "                \n",
        "                if edge_type == 'prerequisite':\n",
        "                    target_course = kg.graph.nodes[target_node].get('code', '')\n",
        "                    query = f\"What is a prerequisite for {start_course}?\"\n",
        "                    answer = f\"{target_course} is a prerequisite for {start_course}.\"\n",
        "                    \n",
        "                    entities = extract_entities_from_query(query)\n",
        "                    subgraph = retriever.retrieve_subgraph(query, entities, max_hops=2)\n",
        "                    graph_context = retriever.format_subgraph_context(subgraph)\n",
        "                    \n",
        "                    synthetic.append({\n",
        "                        'query': query,\n",
        "                        'answer': answer,\n",
        "                        'graph_context': graph_context,\n",
        "                        'reasoning_path': f\"{target_course} -> {start_course}\",\n",
        "                        'type': 'synthetic_prerequisite'\n",
        "                    })\n",
        "    \n",
        "    return synthetic\n",
        "\n",
        "# Generate synthetic questions\n",
        "print(\"Generating synthetic questions...\")\n",
        "synthetic_questions = generate_synthetic_questions_from_graph(kg, num_synthetic=50)\n",
        "print(f\"✅ Generated {len(synthetic_questions)} synthetic questions\")\n",
        "\n",
        "# Augment existing questions\n",
        "augmented = []\n",
        "for q in multi_hop_questions[:20]:\n",
        "    variations = augment_question(q['query'], q['answer'])\n",
        "    for var in variations:\n",
        "        var['graph_context'] = q.get('graph_context', '')\n",
        "        var['reasoning_path'] = q.get('reasoning_path', '')\n",
        "        augmented.append(var)\n",
        "\n",
        "print(f\"✅ Generated {len(augmented)} augmented question variations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"lora_model_kg_qa\")\n",
        "tokenizer.save_pretrained(\"lora_model_kg_qa\")\n",
        "print(\"✅ LoRA adapters saved to 'lora_model_kg_qa'\")\n",
        "\n",
        "# Save knowledge graph\n",
        "import pickle\n",
        "with open(\"kg_graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(kg, f)\n",
        "print(\"✅ Knowledge graph saved to 'kg_graph.pkl'\")\n",
        "\n",
        "# Save retriever (without GAT model for now)\n",
        "with open(\"graph_retriever.pkl\", \"wb\") as f:\n",
        "    pickle.dump(retriever, f)\n",
        "print(\"✅ Graph retriever saved to 'graph_retriever.pkl'\")\n",
        "\n",
        "# Optional: Export merged model\n",
        "# model.save_pretrained_merged(\"merged_model_kg_qa\", tokenizer, save_method=\"merged_16bit\")\n",
        "# print(\"✅ Merged model saved to 'merged_model_kg_qa'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Summary and Next Steps\n",
        "\n",
        "### What We've Built:\n",
        "\n",
        "1. **Knowledge Graph**: Constructed from course data with nodes (courses, professors, topics) and edges (prerequisite, taught_by, covers_topic)\n",
        "2. **Prerequisites & Topics Extraction**: Automated extraction from course descriptions\n",
        "3. **Graph Attention Network**: Framework for graph-based retrieval (GAT model can be trained separately)\n",
        "4. **Multi-hop Reasoning Data**: Generated complex questions requiring graph traversal\n",
        "5. **RAG Training Format**: Combined graph context with LLM training\n",
        "6. **Evaluation Framework**: Metrics for QA evaluation (EM, F1, BLEU, ROUGE)\n",
        "7. **Inference Pipeline**: End-to-end query answering with graph retrieval\n",
        "8. **Data Augmentation**: Synthetic question generation\n",
        "9. **Performance Monitoring**: Tracking graph retrieval statistics\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Train GAT Model**: Implement and train the Graph Attention Network for better graph retrieval scoring\n",
        "2. **Expand Test Set**: Create a comprehensive labeled test set with ground truth answers\n",
        "3. **Fine-tune Hyperparameters**: Optimize GAT and LLM training parameters\n",
        "4. **Add More Relationship Types**: Include degree requirements, course sequences, etc.\n",
        "5. **Improve Topic Extraction**: Use better NLP models for topic extraction\n",
        "6. **Multi-task Learning**: Jointly train graph retrieval and answer generation\n",
        "7. **Attention Visualization**: Show which graph nodes the model attends to\n",
        "8. **Production Deployment**: Create API endpoints for the inference pipeline\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
